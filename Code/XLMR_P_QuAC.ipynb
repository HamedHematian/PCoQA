{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBX9WDkFwsjL"
      },
      "source": [
        "# Libraries & Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UuaR7fKTp_7y"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "! pip install transformers\n",
        "! pip install sentencepiece\n",
        "! gdown \"1rlGSC37ywFxbTBVHaakXot9211TLi3hA\"\n",
        "! unzip xlmr_quac_pretrain.zip\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "files_exist = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qL9GKLsIm1BB"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "! wget -nc https://s3.amazonaws.com/my89public/quac/train_v0.2.json\n",
        "! wget -nc https://s3.amazonaws.com/my89public/quac/val_v0.2.json\n",
        "# ! rm -r examples/train\n",
        "# ! rm -r examples/eval\n",
        "# ! rm -r features/train\n",
        "# ! rm -r features/eval\n",
        "# ! mkdir -p examples/train\n",
        "# ! mkdir -p examples/eval\n",
        "# ! mkdir -p features/train\n",
        "# ! mkdir -p features/eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_tW5BR5tldmp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import json\n",
        "import pickle\n",
        "import unicodedata\n",
        "from tqdm import tqdm\n",
        "from copy import deepcopy\n",
        "import transformers\n",
        "from transformers.optimization import get_linear_schedule_with_warmup\n",
        "from transformers import BertModel, BertForQuestionAnswering, AutoTokenizer, AutoModel\n",
        "from transformers import XLMRobertaTokenizer, XLMRobertaModel, XLMRobertaConfig\n",
        "import os\n",
        "from collections import defaultdict, namedtuple\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import AdamW, Adam, RMSprop\n",
        "from copy import deepcopy\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JcX1yR4GnPO9"
      },
      "outputs": [],
      "source": [
        "def read_file(filename):\n",
        "  with open(filename, 'r') as f:\n",
        "    return json.load(f)\n",
        "\n",
        "def load_data(filename):\n",
        "  with open(filename, 'rb') as f:\n",
        "    x = pickle.load(f)\n",
        "  return x\n",
        "\n",
        "def save_data(data, filename):\n",
        "    with open(filename, \"wb\") as f:\n",
        "        pickle.dump(data, f)\n",
        "\n",
        "train_path = 'train_v0.2.json'\n",
        "eval_path = 'val_v0.2.json'\n",
        "\n",
        "\n",
        "# model_name = 'xlm-roberta-base'\n",
        "model_name = 'xlm-roberta-base'\n",
        "\n",
        "\n",
        "model_path_or_name = model_name\n",
        "\n",
        "# load data\n",
        "train_data = read_file(train_path)\n",
        "eval_data = read_file(eval_path)\n",
        "\n",
        "# load tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path_or_name)\n",
        "model = AutoModel.from_pretrained(model_path_or_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SVkleBAGR7Nd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b26I0rYxR7P6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klvWCqdsSmEB"
      },
      "source": [
        "# Official Evaluation Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1vBDDFWFSrMc"
      },
      "outputs": [],
      "source": [
        "import json, string, re\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "\n",
        "def is_overlapping(x1, x2, y1, y2):\n",
        "  return max(x1, y1) <= min(x2, y2)\n",
        "\n",
        "def normalize_answer(s):\n",
        "  \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
        "  def remove_articles(text):\n",
        "    return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
        "  def white_space_fix(text):\n",
        "    return ' '.join(text.split())\n",
        "  def remove_punc(text):\n",
        "    exclude = set(string.punctuation)\n",
        "    return ''.join(ch for ch in text if ch not in exclude)\n",
        "  def lower(text):\n",
        "    return text.lower()\n",
        "  return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "def f1_score(prediction, ground_truth):\n",
        "  prediction_tokens = normalize_answer(prediction).split()\n",
        "  ground_truth_tokens = normalize_answer(ground_truth).split()\n",
        "  common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
        "  num_same = sum(common.values())\n",
        "  if num_same == 0:\n",
        "    return 0\n",
        "  precision = 1.0 * num_same / len(prediction_tokens)\n",
        "  recall = 1.0 * num_same / len(ground_truth_tokens)\n",
        "  f1 = (2 * precision * recall) / (precision + recall)\n",
        "  return f1\n",
        "\n",
        "def exact_match_score(prediction, ground_truth):\n",
        "  return (normalize_answer(prediction) == normalize_answer(ground_truth))\n",
        "\n",
        "def display_counter(title, c, c2=None):\n",
        "  print(title)\n",
        "  for key, _ in c.most_common():\n",
        "    if c2:\n",
        "      print('%s: %d / %d, %.1f%%, F1: %.1f' % (\n",
        "        key, c[key], sum(c.values()), c[key] * 100. / sum(c.values()), sum(c2[key]) * 100. / len(c2[key])))\n",
        "    else:\n",
        "      print('%s: %d / %d, %.1f%%' % (key, c[key], sum(c.values()), c[key] * 100. / sum(c.values())))\n",
        "\n",
        "def leave_one_out_max(prediction, ground_truths, article):\n",
        "  if len(ground_truths) == 1:\n",
        "    return metric_max_over_ground_truths(prediction, ground_truths, article)[1]\n",
        "  else:\n",
        "    t_f1 = []\n",
        "    # leave out one ref every time\n",
        "    for i in range(len(ground_truths)):\n",
        "      idxes = list(range(len(ground_truths)))\n",
        "      idxes.pop(i)\n",
        "      refs = [ground_truths[z] for z in idxes]\n",
        "      t_f1.append(metric_max_over_ground_truths(prediction, refs, article)[1])\n",
        "  return 1.0 * sum(t_f1) / len(t_f1)\n",
        "\n",
        "\n",
        "def metric_max_over_ground_truths(prediction, ground_truths, article):\n",
        "  scores_for_ground_truths = []\n",
        "  for ground_truth in ground_truths:\n",
        "    score = compute_span_overlap(prediction, ground_truth, article)\n",
        "    scores_for_ground_truths.append(score)\n",
        "  return max(scores_for_ground_truths, key=lambda x: x[1])\n",
        "\n",
        "\n",
        "def handle_cannot(refs):\n",
        "  num_cannot = 0\n",
        "  num_spans = 0\n",
        "  for ref in refs:\n",
        "    if ref == 'CANNOTANSWER':\n",
        "      num_cannot += 1\n",
        "    else:\n",
        "      num_spans += 1\n",
        "  if num_cannot >= num_spans:\n",
        "    refs = ['CANNOTANSWER']\n",
        "  else:\n",
        "    refs = [x for x in refs if x != 'CANNOTANSWER']\n",
        "  return refs\n",
        "\n",
        "\n",
        "def leave_one_out(refs):\n",
        "  if len(refs) == 1:\n",
        "    return 1.\n",
        "  splits = []\n",
        "  for r in refs:\n",
        "    splits.append(r.split())\n",
        "  t_f1 = 0.0\n",
        "  for i in range(len(refs)):\n",
        "    m_f1 = 0\n",
        "    for j in range(len(refs)):\n",
        "      if i == j:\n",
        "        continue\n",
        "      f1_ij = f1_score(refs[i], refs[j])\n",
        "      if f1_ij > m_f1:\n",
        "        m_f1 = f1_ij\n",
        "    t_f1 += m_f1\n",
        "  return t_f1 / len(refs)\n",
        "\n",
        "\n",
        "def compute_span_overlap(pred_span, gt_span, text):\n",
        "  if gt_span == 'CANNOTANSWER':\n",
        "    if pred_span == 'CANNOTANSWER':\n",
        "      return 'Exact match', 1.0\n",
        "    return 'No overlap', 0.\n",
        "  fscore = f1_score(pred_span, gt_span)\n",
        "  pred_start = text.find(pred_span)\n",
        "  gt_start = text.find(gt_span)\n",
        "\n",
        "  if pred_start == -1 or gt_start == -1:\n",
        "    return 'Span indexing error', fscore\n",
        "\n",
        "  pred_end = pred_start + len(pred_span)\n",
        "  gt_end = gt_start + len(gt_span)\n",
        "\n",
        "  fscore = f1_score(pred_span, gt_span)\n",
        "  overlap = is_overlapping(pred_start, pred_end, gt_start, gt_end)\n",
        "\n",
        "  if exact_match_score(pred_span, gt_span):\n",
        "    return 'Exact match', fscore\n",
        "  if overlap:\n",
        "    return 'Partial overlap', fscore\n",
        "  else:\n",
        "    return 'No overlap', fscore\n",
        "\n",
        "\n",
        "def eval_fn(val_results, model_results, verbose):\n",
        "  span_overlap_stats = Counter()\n",
        "  sentence_overlap = 0.\n",
        "  para_overlap = 0.\n",
        "  total_qs = 0.\n",
        "  f1_stats = defaultdict(list)\n",
        "  unfiltered_f1s = []\n",
        "  human_f1 = []\n",
        "  HEQ = 0.\n",
        "  DHEQ = 0.\n",
        "  total_dials = 0.\n",
        "  yes_nos = []\n",
        "  followups = []\n",
        "  unanswerables = []\n",
        "  for p in val_results:\n",
        "    for par in p['paragraphs']:\n",
        "      did = par['id']\n",
        "      qa_list = par['qas']\n",
        "      good_dial = 1.\n",
        "      for qa in qa_list:\n",
        "        q_idx = qa['id']\n",
        "        val_spans = [anss['text'] for anss in qa['answers']]\n",
        "        val_spans = handle_cannot(val_spans)\n",
        "        hf1 = leave_one_out(val_spans)\n",
        "\n",
        "        if did not in model_results or q_idx not in model_results[did]:\n",
        "          # print(did, q_idx, 'no prediction for this dialogue id')\n",
        "          good_dial = 0\n",
        "          f1_stats['NO ANSWER'].append(0.0)\n",
        "          yes_nos.append(False)\n",
        "          followups.append(False)\n",
        "          if val_spans == ['CANNOTANSWER']:\n",
        "            unanswerables.append(0.0)\n",
        "          total_qs += 1\n",
        "          unfiltered_f1s.append(0.0)\n",
        "          if hf1 >= .4:\n",
        "            human_f1.append(hf1)\n",
        "          continue\n",
        "\n",
        "        pred_span, pred_yesno, pred_followup = model_results[did][q_idx]\n",
        "\n",
        "        max_overlap, _ = metric_max_over_ground_truths( \\\n",
        "          pred_span, val_spans, par['context'])\n",
        "        max_f1 = leave_one_out_max( \\\n",
        "          pred_span, val_spans, par['context'])\n",
        "        unfiltered_f1s.append(max_f1)\n",
        "\n",
        "        # dont eval on low agreement instances\n",
        "        if hf1 < .4:\n",
        "          continue\n",
        "\n",
        "        human_f1.append(hf1)\n",
        "        yes_nos.append(pred_yesno == qa['yesno'])\n",
        "        followups.append(pred_followup == qa['followup'])\n",
        "        if val_spans == ['CANNOTANSWER']:\n",
        "          unanswerables.append(max_f1)\n",
        "        if verbose:\n",
        "          print(\"-\" * 20)\n",
        "          print(pred_span)\n",
        "          print(val_spans)\n",
        "          print(max_f1)\n",
        "          print(\"-\" * 20)\n",
        "        if max_f1 >= hf1:\n",
        "          HEQ += 1.\n",
        "        else:\n",
        "          good_dial = 0.\n",
        "        span_overlap_stats[max_overlap] += 1\n",
        "        f1_stats[max_overlap].append(max_f1)\n",
        "        total_qs += 1.\n",
        "      DHEQ += good_dial\n",
        "      total_dials += 1\n",
        "  DHEQ_score = 100.0 * DHEQ / total_dials\n",
        "  HEQ_score = 100.0 * HEQ / total_qs\n",
        "  all_f1s = sum(f1_stats.values(), [])\n",
        "  overall_f1 = 100.0 * sum(all_f1s) / len(all_f1s)\n",
        "  unfiltered_f1 = 100.0 * sum(unfiltered_f1s) / len(unfiltered_f1s)\n",
        "  yesno_score = (100.0 * sum(yes_nos) / len(yes_nos))\n",
        "  followup_score = (100.0 * sum(followups) / len(followups))\n",
        "  unanswerable_score = (100.0 * sum(unanswerables) / len(unanswerables))\n",
        "  metric_json = {\"unfiltered_f1\": unfiltered_f1, \"f1\": overall_f1, \"HEQ\": HEQ_score, \"DHEQ\": DHEQ_score, \"yes/no\": yesno_score, \"followup\": followup_score, \"unanswerable_acc\": unanswerable_score}\n",
        "  if verbose:\n",
        "    print(\"=======================\")\n",
        "    display_counter('Overlap Stats', span_overlap_stats, f1_stats)\n",
        "  print(\"=======================\")\n",
        "  print('Overall F1: %.1f' % overall_f1)\n",
        "  with open('val_report.txt', 'a') as f:\n",
        "    f.write('Overall F1: %.1f' % overall_f1)\n",
        "  print('Yes/No Accuracy : %.1f' % yesno_score)\n",
        "  print('Followup Accuracy : %.1f' % followup_score)\n",
        "  print('Unfiltered F1 ({0:d} questions): {1:.1f}'.format(len(unfiltered_f1s), unfiltered_f1))\n",
        "  print('Accuracy On Unanswerable Questions: {0:.1f} %% ({1:d} questions)'.format(unanswerable_score, len(unanswerables)))\n",
        "  print('Human F1: %.1f' % (100.0 * sum(human_f1) / len(human_f1)))\n",
        "  print('Model F1 >= Human F1 (Questions): %d / %d, %.1f%%' % (HEQ, total_qs, 100.0 * HEQ / total_qs))\n",
        "  print('Model F1 >= Human F1 (Dialogs): %d / %d, %.1f%%' % (DHEQ, total_dials, 100.0 * DHEQ / total_dials))\n",
        "  print(\"=======================\")\n",
        "  output_string = 'Overall F1: %.1f\\n' % overall_f1\n",
        "  output_string += 'Yes/No Accuracy : %.1f\\n' % yesno_score\n",
        "  output_string += 'Followup Accuracy : %.1f\\n' % followup_score\n",
        "  output_string += 'Unfiltered F1 ({0:d} questions): {1:.1f}\\n'.format(len(unfiltered_f1s), unfiltered_f1)\n",
        "  output_string += 'Accuracy On Unanswerable Questions: {0:.1f} %% ({1:d} questions)\\n'.format(unanswerable_score, len(unanswerables))\n",
        "  output_string += 'Human F1: %.1f\\n' % (100.0 * sum(human_f1) / len(human_f1))\n",
        "  output_string += 'Model F1 >= Human F1 (Questions): %d / %d, %.1f%%\\n' % (HEQ, total_qs, 100.0 * HEQ / total_qs)\n",
        "  output_string += 'Model F1 >= Human F1 (Dialogs): %d / %d, %.1f%%' % (DHEQ, total_dials, 100.0 * DHEQ / total_dials)\n",
        "\n",
        "  # save_prediction(epoch, train_step, output_string)\n",
        "\n",
        "  return metric_json\n",
        "\n",
        "def run_eval(filename, labels):\n",
        "  val = json.load(open(labels, 'r'))['data']\n",
        "  preds = defaultdict(dict)\n",
        "  total = 0\n",
        "  val_total = 0\n",
        "  for line in open(filename, 'r'):\n",
        "    if line.strip():\n",
        "      pred_idx = json.loads(line.strip())\n",
        "      dia_id = pred_idx['qid'][0].split(\"_q#\")[0]\n",
        "      for qid, qspan, qyesno, qfollowup in zip(pred_idx['qid'], pred_idx['best_span_str'], pred_idx['yesno'], pred_idx['followup']):\n",
        "        preds[dia_id][qid] = qspan, qyesno, qfollowup\n",
        "        total += 1\n",
        "  for p in val:\n",
        "    for par in p['paragraphs']:\n",
        "      did = par['id']\n",
        "      qa_list = par['qas']\n",
        "      val_total += len(qa_list)\n",
        "  metric_json = eval_fn(val, preds, False)\n",
        "\n",
        "\n",
        "def findall(p, s):\n",
        "    i = s.find(p)\n",
        "    while i != -1:\n",
        "        if i == len(s) - 1:\n",
        "          break\n",
        "        yield i\n",
        "        i = s.find(p, i+1)\n",
        "\n",
        "def _is_control(char):\n",
        "  if char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
        "    print('yes')\n",
        "    return False\n",
        "  cat = unicodedata.category(char)\n",
        "  if cat.startswith(\"C\"):\n",
        "    return True\n",
        "  return False\n",
        "\n",
        "def _is_whitespace(char):\n",
        "  if char == \" \" or char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
        "    return True\n",
        "  cat = unicodedata.category(char)\n",
        "  if cat == \"Zs\":\n",
        "    return True\n",
        "  return False\n",
        "\n",
        "def find_reverse_mapping(answer, context):\n",
        "  global T\n",
        "  global A\n",
        "  founds = []\n",
        "  try:\n",
        "    if answer == '':\n",
        "      return 'CANNOTANSWER'\n",
        "    finds = findall(answer[0], context)\n",
        "  except:\n",
        "    T = context\n",
        "    A = answer\n",
        "    print(answer, 'tttt')\n",
        "    print(context)\n",
        "  for i in finds:\n",
        "    try:\n",
        "      founds.append((i, context[i:i+1]))\n",
        "    except:\n",
        "      pass\n",
        "  for found in founds:\n",
        "    context_index = found[0] + 1\n",
        "    context_start_index = context_index\n",
        "    answer_index = 1\n",
        "    while(True):\n",
        "      if answer_index == len(answer):\n",
        "        return context[context_start_index - 1:context_index]\n",
        "\n",
        "      if context[context_index] == answer[answer_index]:\n",
        "        context_index += 1\n",
        "        answer_index += 1\n",
        "      elif answer[answer_index] == ' ' and _is_whitespace(context[context_index]):\n",
        "        context_index += 1\n",
        "        answer_index += 1\n",
        "      elif _is_control(context[context_index]):\n",
        "        context_index += 1\n",
        "      else:\n",
        "        break\n",
        "\n",
        "def write_to_file(answers, filename):\n",
        "  try:\n",
        "    os.remove(filename)\n",
        "  except:\n",
        "    pass\n",
        "  for dialog_id, answers in answers.items():\n",
        "    res_dict = {\n",
        "        'best_span_str': [],\n",
        "        'qid': [],\n",
        "        'followup': [],\n",
        "        'yesno': []}\n",
        "\n",
        "    for i in range(len(answers)):\n",
        "      qid = dialog_id + '#' + str(i)\n",
        "      res_dict['best_span_str'].append(answers[i])\n",
        "      res_dict['qid'].append(qid)\n",
        "      res_dict['followup'].append('y')\n",
        "      res_dict['yesno'].append('y')\n",
        "\n",
        "    with open(filename,'a') as f:\n",
        "      json.dump(res_dict,f)\n",
        "      f.write('\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZ8K618EwydR"
      },
      "source": [
        "# Preprocess Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJ2HnsMKnFId"
      },
      "outputs": [],
      "source": [
        "class CQA_DATA:\n",
        "\n",
        "  def __init__(self,\n",
        "               question,\n",
        "               context,\n",
        "               history,\n",
        "               answers,\n",
        "               human_answer,\n",
        "               qid,\n",
        "               q_num,\n",
        "               answer_start,\n",
        "               answer_end,\n",
        "               is_answerable):\n",
        "\n",
        "    self.question = question\n",
        "    self.context = context\n",
        "    self.answers = answers\n",
        "    self.human_answer = human_answer\n",
        "    self.history = history\n",
        "    self.qid = qid\n",
        "    self.q_num = q_num\n",
        "    self.answer_start = answer_start\n",
        "    self.answer_end = answer_end\n",
        "    self.is_answerable = is_answerable\n",
        "    self.cleaned_context = None\n",
        "    self.answer = self.answers[0]['text']\n",
        "\n",
        "  def __repr__(self):\n",
        "    repr = ''\n",
        "    repr += 'context -> ' + self.context[:100] + '\\n'\n",
        "    repr += 'question ->' + self.question + '\\n'\n",
        "    repr += 'question id ->' + str(self.qid) + '\\n'\n",
        "    repr += 'turn_number ->' + str(self.turn_number) + '\\n'\n",
        "    repr += 'answer ->' + self.answers[0]['text'] + '\\n'\n",
        "    return repr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UbyqY9tE1S7-"
      },
      "outputs": [],
      "source": [
        "class Feature:\n",
        "\n",
        "  def __init__(self,\n",
        "               qid,\n",
        "               question_part,\n",
        "               input_ids,\n",
        "               attention_mask,\n",
        "               offset_mappings,\n",
        "               max_context_dict,\n",
        "               start,\n",
        "               end,\n",
        "               is_answerable,\n",
        "               context,\n",
        "               cleaned_context,\n",
        "               context_start,\n",
        "               context_end,\n",
        "               example_start_char,\n",
        "               example_end_char,\n",
        "               example_answer):\n",
        "\n",
        "    self.qid = qid\n",
        "    self.question_part = question_part\n",
        "    self.input_ids = input_ids\n",
        "    self.attention_mask = attention_mask\n",
        "    self.offset_mappings = offset_mappings\n",
        "    self.max_context_dict = max_context_dict\n",
        "    self.start = start\n",
        "    self.end = end\n",
        "    self.is_answerable = is_answerable\n",
        "    self.context = context\n",
        "    self.cleaned_context = cleaned_context\n",
        "    self.context_start = context_start\n",
        "    self.context_end = context_end\n",
        "    self.example_start_char = example_start_char\n",
        "    self.example_end_char = example_end_char\n",
        "    self.example_answer = example_answer\n",
        "\n",
        "  def __repr__(self):\n",
        "    repr = ''\n",
        "    repr += 'qid --> ' + str(self.qid) + '\\n'\n",
        "    repr += 'quesion part --> ' + str(self.question_part) + '\\n'\n",
        "    repr += 'answer part --> ' + str(self.start) + ' ' + str(self.end) + '\\n'\n",
        "    return repr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p5EndDCQst27"
      },
      "outputs": [],
      "source": [
        "class CleanSample():\n",
        "\n",
        "  def __init__(self, sample):\n",
        "    self.sample = sample\n",
        "\n",
        "  def clean(self):\n",
        "    cleaned_question = self._clean_text(self.sample.question)\n",
        "    cleaned_context = self._clean_text(self.sample.context, keep_track=True)\n",
        "    self.sample.question = cleaned_question\n",
        "    self.sample.cleaned_context = cleaned_context\n",
        "    cleaned_answer = self.strip_text(self.sample.answers[0]['text'])\n",
        "    if cleaned_answer[1] != 0 or cleaned_answer[2] != 0:\n",
        "      print('yes')\n",
        "    self.sample.cleaned_answer = {\n",
        "        'text': cleaned_answer[0],\n",
        "        'start': self.sample.answers[0]['start'] + cleaned_answer[1],\n",
        "        'end': self.sample.answers[0]['end'] - cleaned_answer[2]\n",
        "    }\n",
        "    return self.sample\n",
        "\n",
        "  def _is_control(self, char):\n",
        "    if char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
        "      return False\n",
        "    cat = unicodedata.category(char)\n",
        "    if cat.startswith(\"C\"):\n",
        "      return True\n",
        "    return False\n",
        "\n",
        "  def strip_text(self, text):\n",
        "    start_spaces = 0\n",
        "    end_spaces = 0\n",
        "    while(text[0] == ' '):\n",
        "      text = text[1:]\n",
        "      start_spaces += 1\n",
        "    while(text[-1] == ' '):\n",
        "      text = text[:-1]\n",
        "      end_spaces += 1\n",
        "    return text, start_spaces, end_spaces\n",
        "\n",
        "\n",
        "  def _is_whitespace(self, char):\n",
        "    if char == \" \" or char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
        "      return True\n",
        "    cat = unicodedata.category(char)\n",
        "    if cat == \"Zs\":\n",
        "      return True\n",
        "    return False\n",
        "\n",
        "  def _clean_text(self, text, keep_track=False):\n",
        "    output = []\n",
        "    new_answer_start = self.sample.answer_start\n",
        "    new_answer_end = self.sample.answer_end\n",
        "\n",
        "    for index, char in enumerate(text):\n",
        "      cp = ord(char)\n",
        "      if cp == 0 or cp == 0xfffd or self._is_control(char):\n",
        "        if keep_track:\n",
        "          if index < new_answer_start:\n",
        "            new_answer_start -= 1\n",
        "          if index < new_answer_end:\n",
        "            new_answer_end -= 1\n",
        "        continue\n",
        "      if self._is_whitespace(char):\n",
        "        output.append(\" \")\n",
        "      else:\n",
        "        output.append(char)\n",
        "    self.sample.answer_start = new_answer_start\n",
        "    self.sample.answer_end = new_answer_end\n",
        "    return \"\".join(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfqAND5NOWgD"
      },
      "source": [
        "# Examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mCTy21ProL3j"
      },
      "outputs": [],
      "source": [
        "def make_examples(data, data_type, num_sample, clean_samples=True):\n",
        "  examples = []\n",
        "  each_file_size = 1000\n",
        "  example_file_index = 0\n",
        "  data_dir = f'examples/{data_type}/'\n",
        "\n",
        "  for dialog_num, sample in enumerate(tqdm(data['data'][:num_sample], leave=False, position=0)):\n",
        "    dialog_history = []\n",
        "    dialog_container = []\n",
        "    dialog = sample['paragraphs'][0]\n",
        "    context = dialog['context']\n",
        "    dialog_len = len(dialog['qas'])\n",
        "\n",
        "    for q_num, qas in enumerate(dialog['qas']):\n",
        "      history = []\n",
        "      question = qas['question']\n",
        "      human_answer = qas['orig_answer']\n",
        "      qid = qas['id']\n",
        "      answers = []\n",
        "\n",
        "      for answer in qas['answers']:\n",
        "        answer_ = {}\n",
        "        answer_['text'] = answer['text']\n",
        "        answer_['start'] = answer['answer_start']\n",
        "        answer_['end'] = answer['answer_start'] + len(answer['text'])\n",
        "        answers.append(answer_)\n",
        "\n",
        "\n",
        "      is_answerable = False if qas['answers'][0]['text'] == 'CANNOTANSWER' else True\n",
        "\n",
        "      if not q_num == 0:\n",
        "        history = deepcopy(dialog_history)\n",
        "\n",
        "      cqa_example = CQA_DATA(question=question,\n",
        "                             context=context,\n",
        "                             history=history,\n",
        "                             answers=answers,\n",
        "                             human_answer=human_answer,\n",
        "                             qid=qid,\n",
        "                             q_num=q_num,\n",
        "                             answer_start=answers[0]['start'],\n",
        "                             answer_end=answers[0]['end'],\n",
        "                             is_answerable=is_answerable)\n",
        "\n",
        "      cqa_example = CleanSample(cqa_example).clean() if clean_samples else cqa_example\n",
        "      examples.append(cqa_example)\n",
        "      dialog_history.append(cqa_example)\n",
        "\n",
        "    if (dialog_num + 1) % each_file_size == 0:\n",
        "      filename = f'{data_type}_examples_' + str(example_file_index) + '.bin'\n",
        "      save_data(examples, os.path.join(data_dir, filename))\n",
        "      example_file_index += 1\n",
        "      examples = []\n",
        "\n",
        "  if examples != []:\n",
        "    filename = f'{data_type}_examples_' + str(example_file_index) + '.bin'\n",
        "    save_data(examples, os.path.join(data_dir, filename))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bA1ICM6HOZJ-"
      },
      "source": [
        "# Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G07euhk_P_Ri"
      },
      "outputs": [],
      "source": [
        "def make_features(data_type):\n",
        "  data_dir = f'examples/{data_type}/'\n",
        "  example_files = os.listdir(data_dir)\n",
        "  example_files = [os.path.join(data_dir, example_file) for example_file in example_files]\n",
        "  features_list = []\n",
        "  features_dir = f'features/{data_type}/'\n",
        "  current_file_index = 0\n",
        "  max_history_to_consider = 11\n",
        "\n",
        "  for file_index, filename in enumerate(example_files):\n",
        "    examples = load_data(filename)\n",
        "    for example in tqdm(examples, leave=False, position=0):\n",
        "      example_features = []\n",
        "      concatenated_question = []\n",
        "\n",
        "      # concat history\n",
        "      for hist in example.history[-max_history_to_consider:]:\n",
        "        concatenated_question.append(hist.question)\n",
        "\n",
        "      # append current question to concatenated question\n",
        "      concatenated_question.append(example.question)\n",
        "\n",
        "      # make string out of concatenated question\n",
        "      concatenated_question = ' '.join(concatenated_question)\n",
        "\n",
        "      # tokenize current feature\n",
        "      text_tokens = tokenizer(\n",
        "          concatenated_question,\n",
        "          example.cleaned_context,\n",
        "          max_length=model.config.max_position_embeddings - 2,\n",
        "          padding='max_length',\n",
        "          truncation='only_second',\n",
        "          return_overflowing_tokens=True,\n",
        "          return_offsets_mapping=True,\n",
        "          stride=128)\n",
        "\n",
        "      # find start and end of context\n",
        "      for idx in range(len(text_tokens['input_ids'])):\n",
        "        found_start = False\n",
        "        found_end = False\n",
        "        context_start = 0\n",
        "        context_end = 511\n",
        "        max_context_dict = {}\n",
        "\n",
        "        for token_idx, token in enumerate(text_tokens['offset_mapping'][idx][1:]):\n",
        "          if token[0] == 0 and token[1] == 0:\n",
        "            context_start = token_idx + 3\n",
        "            break\n",
        "\n",
        "        for token_idx, token in enumerate(text_tokens['offset_mapping'][idx][context_start:]):\n",
        "          if token[0] == 0 and token[1] == 0:\n",
        "            context_end = token_idx + context_start - 1\n",
        "            break\n",
        "\n",
        "        chunk_offset_mapping = text_tokens['offset_mapping'][idx]\n",
        "        for context_idx, data in enumerate(chunk_offset_mapping[context_start: context_end + 1]):\n",
        "          max_context_dict[f'({data[0]},{data[1]})'] = min(context_idx, context_end - context_idx) + (context_end - context_start + 1) * .01\n",
        "\n",
        "        # find and mark current question answer\n",
        "        marker_ids = np.zeros(shape=(model.config.max_position_embeddings,), dtype=np.int64)\n",
        "        last_token = None\n",
        "        for token_idx, token in enumerate(chunk_offset_mapping[context_start: context_end + 1]):\n",
        "          if token[0] == example.cleaned_answer['start'] and not found_start:\n",
        "            found_start = True\n",
        "            start = token_idx + context_start\n",
        "\n",
        "          elif last_token and last_token[0] < example.cleaned_answer['start'] and token[0] > example.cleaned_answer['start']:\n",
        "            found_start = True\n",
        "            start = (token_idx - 1) + context_start\n",
        "\n",
        "          if token[1] == example.cleaned_answer['end'] and not found_end:\n",
        "            found_end = True\n",
        "            end = token_idx + context_start\n",
        "\n",
        "          elif last_token and last_token[1] < example.cleaned_answer['end'] and token[1] > example.cleaned_answer['end'] and last_token:\n",
        "            found_end = True\n",
        "            end = token_idx + context_start\n",
        "          last_token = token\n",
        "\n",
        "        # add feature to features list\n",
        "        if end < start and found_start and found_end:\n",
        "          assert False, 'start and end do not match'\n",
        "\n",
        "        # since there is no prediction we throw the example out (only when training)\n",
        "        # if ((not found_start) or (not found_end)) and data_type == 'train':\n",
        "        #   continue\n",
        "\n",
        "        if ((not found_start) or (not found_end)):\n",
        "          continue\n",
        "          start, end = 0, 0\n",
        "          if example.is_answerable == False:\n",
        "            print(example.answers[0]['text'])\n",
        "\n",
        "        # plausibility check\n",
        "        if found_start or found_end:\n",
        "          answer = example.cleaned_answer['text'].strip()\n",
        "          generated_answer = example.cleaned_context[chunk_offset_mapping[start][0]: chunk_offset_mapping[end][1]]\n",
        "          if answer.find(generated_answer) == -1:\n",
        "            print('some inner')\n",
        "\n",
        "        # mark history answers\n",
        "\n",
        "        example_features.append(Feature(example.qid,\n",
        "                                          idx,\n",
        "                                          text_tokens['input_ids'][idx],\n",
        "                                          text_tokens['attention_mask'][idx],\n",
        "                                          text_tokens['offset_mapping'][idx],\n",
        "                                          max_context_dict,\n",
        "                                          start,\n",
        "                                          end,\n",
        "                                          example.is_answerable,\n",
        "                                          example.context,\n",
        "                                          example.cleaned_context,\n",
        "                                          context_start,\n",
        "                                          context_end,\n",
        "                                          example.answer_start,\n",
        "                                          example.answer_end,\n",
        "                                          example.answer))\n",
        "      # create max context mask\n",
        "      for feature_1 in example_features:\n",
        "        max_context_mask = {}\n",
        "        for key in list(feature_1.max_context_dict.keys()):\n",
        "          max_context_mask[key] = True\n",
        "          for feature_2 in example_features:\n",
        "            if key in feature_2.max_context_dict:\n",
        "              if feature_1.max_context_dict[key] < feature_2.max_context_dict[key]:\n",
        "                max_context_mask[key] = False\n",
        "        feature_1.max_context_mask = max_context_mask\n",
        "\n",
        "        found_start = found_end = False\n",
        "        start_mask = end_mask = 0\n",
        "        # now compute span mask\n",
        "        for key_idx, (key, value) in enumerate(feature_1.max_context_mask.items()):\n",
        "          if key_idx == 0 and value:\n",
        "            found_start = True\n",
        "          elif value and not found_start:\n",
        "            start_mask = key_idx\n",
        "            found_start = True\n",
        "          elif not value and found_start and not found_end:\n",
        "            end_mask = key_idx\n",
        "            found_end = True\n",
        "          elif key_idx == len(feature_1.max_context_mask) - 1 and value and not found_end:\n",
        "            end_mask = key_idx + 1\n",
        "        feature_1.mask_span = [context_start + start_mask, context_start + end_mask]\n",
        "      features_list.extend(example_features)\n",
        "\n",
        "    filename = f'{data_type}_features_' + str(file_index) + '.bin'\n",
        "    save_data(features_list, os.path.join(features_dir, filename))\n",
        "    features_list = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QGL-bxcJPDTj"
      },
      "outputs": [],
      "source": [
        "if not files_exist:\n",
        "  make_examples(train_data, 'train', 10000000)\n",
        "  make_examples(eval_data, 'eval', 100000000000)\n",
        "  make_features('train')\n",
        "  make_features('eval')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zcIndEQYUlzE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63441069-2631-48d8-f64e-77d5bdb0143a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: features/ (stored 0%)\n",
            "  adding: features/eval/ (stored 0%)\n",
            "  adding: features/eval/eval_features_0.bin (deflated 79%)\n",
            "  adding: features/train/ (stored 0%)\n",
            "  adding: features/train/train_features_1.bin (deflated 80%)\n",
            "  adding: features/train/train_features_10.bin (deflated 80%)\n",
            "  adding: features/train/train_features_8.bin (deflated 80%)\n",
            "  adding: features/train/train_features_2.bin (deflated 80%)\n",
            "  adding: features/train/train_features_6.bin (deflated 80%)\n",
            "  adding: features/train/train_features_4.bin (deflated 80%)\n",
            "  adding: features/train/train_features_9.bin (deflated 80%)\n",
            "  adding: features/train/train_features_11.bin (deflated 80%)\n",
            "  adding: features/train/train_features_0.bin (deflated 80%)\n",
            "  adding: features/train/train_features_5.bin (deflated 81%)\n",
            "  adding: features/train/train_features_3.bin (deflated 80%)\n",
            "  adding: features/train/train_features_7.bin (deflated 80%)\n"
          ]
        }
      ],
      "source": [
        "# ! zip -r xlmr_quac_pretrain.zip features/\n",
        "# ! cp xlmr_quac_pretrain.zip drive/MyDrive/PCoQA_Data/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxFLOtgbwibI"
      },
      "source": [
        "# DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TxJWQlfxcXUP"
      },
      "outputs": [],
      "source": [
        "class DataManager:\n",
        "\n",
        "  def __init__(self, current_file, current_index, data_dir, batch_size, shuffle=True):\n",
        "    self.files = sorted(os.listdir(data_dir), key=lambda x: int(x.split('_')[2].split('.')[0]))\n",
        "    self.files = list(map(lambda x: os.path.join(data_dir, x), self.files))\n",
        "    self.shuffle = shuffle\n",
        "    self.data_len = 0\n",
        "    for filename in self.files:\n",
        "      self.data_len += len(load_data(filename))\n",
        "    self.batch_size = batch_size\n",
        "    self.reset_datamanager(current_file, current_index)\n",
        "\n",
        "  def reset_datamanager(self, current_file_index, current_index):\n",
        "    self.current_index = current_index\n",
        "    self.current_file_index = current_file_index\n",
        "    self.features = self.load_data_file(self.files[self.current_file_index])\n",
        "\n",
        "  def load_data_file(self, filename):\n",
        "    if self.shuffle:\n",
        "      data = load_data(filename)\n",
        "      random.shuffle(data)\n",
        "      return data\n",
        "    else:\n",
        "      return load_data(filename)\n",
        "\n",
        "  def next(self):\n",
        "    temp = self.features[self.current_index:self.current_index + self.batch_size]\n",
        "    self.temp = temp\n",
        "    self.current_index += self.batch_size\n",
        "    if self.current_index >= len(self.features):\n",
        "      self.current_index = 0\n",
        "      self.current_file_index += 1\n",
        "      if self.current_file_index == len(self.files):\n",
        "        self.reset_datamanager(current_file_index=0, current_index=0)\n",
        "        return temp, True\n",
        "      else:\n",
        "        self.features = self.load_data_file(self.files[self.current_file_index])\n",
        "    return temp, False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "snSdz65TdB11"
      },
      "outputs": [],
      "source": [
        "class DataLoader:\n",
        "\n",
        "  def __init__(self, current_file, current_index, batch_size, shuffle=True, training=True):\n",
        "    data_type = 'train' if training else 'eval'\n",
        "    self.batch_size = batch_size\n",
        "    self.data_manager = DataManager(current_file, current_index, f'features/{data_type}/', batch_size, shuffle)\n",
        "\n",
        "  def __iter__(self):\n",
        "    self.stop_iteration = False\n",
        "    return self\n",
        "\n",
        "  def __len__(self):\n",
        "    return int(self.data_manager.data_len // self.batch_size)\n",
        "\n",
        "  def reset_dataloader(self, current_file, current_index):\n",
        "    self.data_manager.reset_datamanager(current_file, current_index)\n",
        "\n",
        "  def features_2_tensor(self, features):\n",
        "    x = dict()\n",
        "    x['input_ids'] = torch.LongTensor([feature.input_ids for feature in features])\n",
        "    x['attention_mask'] = torch.LongTensor([feature.attention_mask for feature in features])\n",
        "    #x['token_type_ids'] = torch.LongTensor([feature.token_type_ids for feature in features])\n",
        "    x['start_positions'] = torch.cat([torch.tensor([feature.start]) for feature in features]).view(-1)\n",
        "    x['end_positions'] = torch.cat([torch.tensor([feature.end]) for feature in features]).view(-1)\n",
        "    x['features'] = features\n",
        "    return x\n",
        "\n",
        "  def __next__(self):\n",
        "    if self.stop_iteration:\n",
        "      raise StopIteration\n",
        "    features, self.stop_iteration = self.data_manager.next()\n",
        "    return self.features_2_tensor(features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xP75pmjbOGW"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lxfai3OuiuzP"
      },
      "outputs": [],
      "source": [
        "feature_output = namedtuple(\n",
        "    'feature_output',\n",
        "        ['start_logit', 'end_logit', 'feature'])\n",
        "\n",
        "PrelimPrediction = namedtuple(\n",
        "    \"PrelimPrediction\",\n",
        "        [\"feature_index\", \"start_index\", \"end_index\", \"start_logit\", \"end_logit\", \"qid\"]\n",
        "    )\n",
        "\n",
        "NbestPrediction = namedtuple(\n",
        "    \"NbestPrediction\", [\"text\", \"start_logit\", \"end_logit\"]\n",
        ")\n",
        "\n",
        "Answer = namedtuple(\n",
        "    'Answer', ['qid', 'answer']\n",
        ")\n",
        "\n",
        "def to_numpy(tensor):\n",
        "  return tensor.detach().cpu().numpy()\n",
        "\n",
        "class EvalProcessOutput:\n",
        "  def __init__(self, n_best_size=4, answer_max_len=40, answerability_threshold=0.0):\n",
        "    self.answers = defaultdict(list)\n",
        "    self.examples_output = []\n",
        "    self.n_best_size = n_best_size\n",
        "    self.answer_max_len = answer_max_len\n",
        "    self.answerability_threshold = answerability_threshold\n",
        "    self.ps = []\n",
        "\n",
        "\n",
        "  def process_feature_output(self, start_logits, end_logits, features):\n",
        "    for start_logit, end_logit, feature in zip(start_logits, end_logits, features):\n",
        "      self.examples_output.append(\n",
        "          feature_output(start_logit, end_logit, feature)\n",
        "      )\n",
        "\n",
        "  def stack_features(self):\n",
        "    examples_list = defaultdict(list)\n",
        "    for feature_out in self.examples_output:\n",
        "      examples_list[feature_out.feature.qid].append(feature_out)\n",
        "    return examples_list\n",
        "\n",
        "\n",
        "  def process_output(self):\n",
        "    self.extract_answers()\n",
        "    self.get_predictions()\n",
        "\n",
        "  def get_predictions(self):\n",
        "    dialogs = defaultdict(list)\n",
        "    self.dialogs_answers = defaultdict(list)\n",
        "    for example_qid, answer in self.answers.items():\n",
        "      dialog_id = example_qid.split('#')[0]\n",
        "      dialogs[dialog_id].append(Answer(example_qid, answer))\n",
        "\n",
        "    self.digs = dialogs\n",
        "    for dialog_id, dialog in dialogs.items():\n",
        "      dialog = sorted(dialog, key=lambda x: int(x.qid.split('#')[1]))\n",
        "      max_dialog_len = int(dialog[-1].qid.split('#')[1]) + 1\n",
        "      self.dialogs_answers[dialog_id] = ['' for i in range(max_dialog_len)]\n",
        "      for example in dialog:\n",
        "        example_turn = int(example.qid.split('#')[1])\n",
        "        self.dialogs_answers[dialog_id][example_turn] = example.answer\n",
        "\n",
        "\n",
        "  def extract_answers(self):\n",
        "    examples_list = self.stack_features()\n",
        "    for example_qid, example in examples_list.items():\n",
        "      null_score = np.inf\n",
        "      prelim_predictions = []\n",
        "      self.example = example\n",
        "      for feature_index, feature_output in enumerate(example):\n",
        "        feature_null_score = feature_output.start_logit[0] + feature_output.end_logit[0]\n",
        "\n",
        "        if feature_null_score < null_score:\n",
        "          null_score = feature_null_score\n",
        "          null_feature_index = feature_index\n",
        "          null_start_logit = feature_output.start_logit[0]\n",
        "          null_end_logit = feature_output.end_logit[0]\n",
        "\n",
        "        start_indexes = self.get_best_indexes(feature_output.start_logit)\n",
        "        end_indexes = self.get_best_indexes(feature_output.end_logit)\n",
        "\n",
        "        for start_index in start_indexes:\n",
        "          for end_index in end_indexes:\n",
        "            if start_index > feature_output.feature.context_end:\n",
        "              continue\n",
        "            # if end_index > feature_output.feature.context_end:\n",
        "            #   continue\n",
        "            # if start_index < feature_output.feature.context_start:\n",
        "            #   continue\n",
        "            if end_index < feature_output.feature.context_start:\n",
        "              continue\n",
        "            if start_index < feature_output.feature.mask_span[0]:\n",
        "              continue\n",
        "            if end_index - start_index + 1 > self.answer_max_len:\n",
        "              continue\n",
        "            if end_index <= start_index:\n",
        "              continue\n",
        "\n",
        "            prelim_predictions.append(\n",
        "                PrelimPrediction(\n",
        "                  feature_index=feature_index,\n",
        "                  start_index=start_index,\n",
        "                  end_index=end_index,\n",
        "                  start_logit=feature_output.start_logit[start_index],\n",
        "                  end_logit=feature_output.end_logit[end_index],\n",
        "                  qid=example_qid\n",
        "            )\n",
        "                )\n",
        "      # append a null one for handling CANNOTANSWER\n",
        "      prelim_predictions.append(\n",
        "        PrelimPrediction(\n",
        "          feature_index=null_feature_index,\n",
        "          start_index=0,\n",
        "          end_index=0,\n",
        "          start_logit=null_start_logit,\n",
        "          end_logit=null_end_logit,\n",
        "          qid=example_qid\n",
        "      )\n",
        "        )\n",
        "      prelim_predictions = sorted(prelim_predictions, key=lambda x: (x.start_logit + x.end_logit), reverse=True)\n",
        "      self.t = prelim_predictions\n",
        "      # print(ff)\n",
        "      best_pred = prelim_predictions[0]\n",
        "      is_answerable = null_score - (best_pred.start_logit + best_pred.end_logit) <= self.answerability_threshold\n",
        "      if is_answerable:\n",
        "        feature = example[best_pred.feature_index].feature\n",
        "        start_char = feature.offset_mappings[best_pred.start_index][0]\n",
        "        end_char = feature.offset_mappings[best_pred.end_index][1]\n",
        "        cleaned_answer = feature.cleaned_context[start_char: end_char + 1]\n",
        "        answer = find_reverse_mapping(cleaned_answer, feature.context)\n",
        "        # answer = self.improve_answer_quality(answer)\n",
        "      else:\n",
        "        answer = 'CANNOTANSWER'\n",
        "\n",
        "      self.answers[example_qid] = answer\n",
        "\n",
        "\n",
        "  def get_best_indexes(self, logits):\n",
        "    index_and_score = sorted(enumerate(logits), key=lambda x: x[1], reverse=True)\n",
        "    best_indexes = []\n",
        "    for i in range(len(index_and_score)):\n",
        "        if i >= self.n_best_size:\n",
        "            break\n",
        "        best_indexes.append(index_and_score[i][0])\n",
        "    return best_indexes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RMu9bqfTGXb"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MH9inpN7THfx"
      },
      "outputs": [],
      "source": [
        "class BertHAE(nn.Module):\n",
        "\n",
        "  def __init__(self, bert, device):\n",
        "    super(BertHAE, self).__init__()\n",
        "    self.transformer = bert\n",
        "    self.start_end_head = nn.Linear(self.transformer.config.hidden_size, 2)\n",
        "    nn.init.normal_(self.start_end_head.weight, mean=.0, std=.02)\n",
        "    self.device = device\n",
        "\n",
        "  def forward(self, x):\n",
        "    for key in x:\n",
        "      x[key] = x[key].to(device)\n",
        "    # transformer output\n",
        "    transformer_output = self.transformer(**x)\n",
        "    start_end_logits = self.start_end_head(transformer_output.last_hidden_state)\n",
        "    start_logits, end_logits = start_end_logits.split(1, dim=-1)\n",
        "    start_logits = start_logits.squeeze(-1)\n",
        "    end_logits = end_logits.squeeze(-1)\n",
        "    return start_logits, end_logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVEJYM1Z4drU"
      },
      "source": [
        "# Saving Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-S1TjgAc4gNp",
        "outputId": "d671a1f4-5004-428e-ed23-c0a9537377af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "crated saved dir\n",
            "No checkpoint found, training from begining\n"
          ]
        }
      ],
      "source": [
        "drive_prefix = 'drive/MyDrive/XLMR_P_QuAC/'\n",
        "drive_checkpoint_dir = 'Checkpoint/'\n",
        "drive_log_dir = 'Log/'\n",
        "checkpoint_dir = os.path.join(drive_prefix, drive_checkpoint_dir)\n",
        "log_dir = os.path.join(drive_prefix, drive_log_dir)\n",
        "\n",
        "meta_log_file = os.path.join(drive_prefix, drive_log_dir, 'test.txt')\n",
        "prediction_file_prefix = os.path.join(drive_prefix, drive_log_dir, 'prediction_')\n",
        "loss_log_file = os.path.join(drive_prefix, drive_log_dir, 'loss.txt')\n",
        "mean_f1_file = os.path.join(drive_prefix, drive_log_dir, 'mean_f1.txt')\n",
        "\n",
        "if not os.path.exists(drive_prefix):\n",
        "  os.mkdir(drive_prefix)\n",
        "  print('crated saved dir')\n",
        "if not os.path.exists(checkpoint_dir):\n",
        "  os.mkdir(checkpoint_dir)\n",
        "if not os.path.exists(log_dir):\n",
        "  os.mkdir(log_dir)\n",
        "\n",
        "with open(meta_log_file, 'w') as f:\n",
        "  pass\n",
        "# check if drive is accessible\n",
        "try:\n",
        "   with open(os.path.join(drive_prefix, drive_log_dir, 'test.txt'), 'r') as f:\n",
        "      pass\n",
        "except:\n",
        "  print('No Access to Drive')\n",
        "  exit()\n",
        "\n",
        "def print_loss(loss_collection, epoch, step):\n",
        "  txt = f'EPOCH [{epoch + 1}/{epochs}] | STEP [{step}/{int(len(train_dataloader) / accumulation_steps)}] | Loss {round(sum(loss_collection) / len(loss_collection), 4)}'\n",
        "  print(txt)\n",
        "\n",
        "def save_loss(loss_collection, epoch, step):\n",
        "  txt = f'EPOCH [{epoch + 1}/{epochs}] | STEP [{step}/{int(len(train_dataloader) / accumulation_steps)}] | Loss {round(sum(loss_collection) / len(loss_collection), 4)}'\n",
        "  with open(loss_log_file, 'a') as f:\n",
        "    f.write(txt)\n",
        "    f.write('\\n')\n",
        "\n",
        "\n",
        "\n",
        "# check the checkpoints drive\n",
        "checkpoint_files = os.listdir(os.path.join(drive_prefix, drive_checkpoint_dir))\n",
        "if len(checkpoint_files) == 0:\n",
        "  checkpoint_available = False\n",
        "  print('No checkpoint found, training from begining')\n",
        "else:\n",
        "  checkpoint_available = True\n",
        "  assert len(checkpoint_files) >= 1, 'Checkpoints are messed up'\n",
        "\n",
        "if checkpoint_available:\n",
        "  current_checkpoint = sorted(checkpoint_files, key=lambda x: [int(x.split('_')[1]), int(x.split('_')[2]), int(x.split('_')[3])])[-1]\n",
        "  print('checkpoints to load ', current_checkpoint)\n",
        "  current_checkpoint = os.path.join(drive_prefix, drive_checkpoint_dir, current_checkpoint)\n",
        "\n",
        "\n",
        "def save_prediction(epoch, step, prediction_log):\n",
        "  with open(os.path.join(drive_prefix, drive_log_dir, 'prediction.txt'), 'a') as f:\n",
        "    f.write(f'--------- EPOCH {epoch} STEP {step} ---------\\n')\n",
        "    f.write(prediction_log)\n",
        "    f.write('\\n')\n",
        "    f.write('\\n')\n",
        "\n",
        "def save_checkpoint(epoch, current_file, current_index):\n",
        "  filename_prefix = os.path.join(drive_prefix, drive_checkpoint_dir, f'checkpoint_{epoch}_{current_file}_{current_index}')\n",
        "  checkpoint_config = {\n",
        "  'epoch': epoch,\n",
        "  'step': train_step,\n",
        "  'optimizer_dict': optimizer.state_dict(),\n",
        "  'scheduler_dict': scheduler.state_dict(),\n",
        "  'model_dict': berthae.state_dict(),\n",
        "  'train_current_file': current_file,\n",
        "  'train_current_index': current_index}\n",
        "  torch.save(checkpoint_config, filename_prefix)\n",
        "\n",
        "def load_checkpoint():\n",
        "    # models have been loaded before so no need to load them again\n",
        "    checkpoint_config = torch.load(current_checkpoint)\n",
        "    return (checkpoint_config['epoch'],\n",
        "            checkpoint_config['step'],\n",
        "            checkpoint_config['optimizer_dict'],\n",
        "            checkpoint_config['scheduler_dict'],\n",
        "            checkpoint_config['model_dict'],\n",
        "            checkpoint_config['train_current_file'],\n",
        "            checkpoint_config['train_current_index'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiS6rtcd3zDS"
      },
      "source": [
        "# Train loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0XUAmIcT0wrk",
        "outputId": "06585cfd-63a9-4834-992c-5e38b80c360f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH [1/3] | STEP [100/406] | Loss 5.5579\n",
            "EPOCH [1/3] | STEP [199/406] | Loss 4.5743\n",
            "EPOCH [1/3] | STEP [298/406] | Loss 4.2354\n",
            "EPOCH [1/3] | STEP [397/406] | Loss 4.1192\n",
            "-------------------- Evaluation --------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-18-d40427c6f3de>\", line 103, in <cell line: 46>\n",
            "    run_eval(filename, 'val_v0.2.json')\n",
            "  File \"<ipython-input-5-d20932a00182>\", line 252, in run_eval\n",
            "    metric_json = eval_fn(val, preds, False)\n",
            "  File \"<ipython-input-5-d20932a00182>\", line 150, in eval_fn\n",
            "    hf1 = leave_one_out(val_spans)\n",
            "  File \"<ipython-input-5-d20932a00182>\", line 94, in leave_one_out\n",
            "    f1_ij = f1_score(refs[i], refs[j])\n",
            "  File \"<ipython-input-5-d20932a00182>\", line 23, in f1_score\n",
            "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
            "  File \"<ipython-input-5-d20932a00182>\", line 19, in normalize_answer\n",
            "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
            "  File \"<ipython-input-5-d20932a00182>\", line 11, in remove_articles\n",
            "    return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
            "  File \"/usr/lib/python3.10/re.py\", line 209, in sub\n",
            "    return _compile(pattern, flags).sub(repl, string, count)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 1662, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 1620, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 829, in getsourcefile\n",
            "    module = getmodule(object, filename)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 878, in getmodule\n",
            "    os.path.realpath(f)] = module.__name__\n",
            "  File \"/usr/lib/python3.10/posixpath.py\", line 396, in realpath\n",
            "    path, ok = _joinrealpath(filename[:0], filename, strict, {})\n",
            "  File \"/usr/lib/python3.10/posixpath.py\", line 431, in _joinrealpath\n",
            "    st = os.lstat(newpath)\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-d40427c6f3de>\u001b[0m in \u001b[0;36m<cell line: 46>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0mwrite_to_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdialogs_answers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m   \u001b[0mrun_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'val_v0.2.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m   \u001b[0mberthae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-d20932a00182>\u001b[0m in \u001b[0;36mrun_eval\u001b[0;34m(filename, labels)\u001b[0m\n\u001b[1;32m    251\u001b[0m       \u001b[0mval_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqa_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m   \u001b[0mmetric_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-d20932a00182>\u001b[0m in \u001b[0;36meval_fn\u001b[0;34m(val_results, model_results, verbose)\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mval_spans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle_cannot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_spans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mhf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mleave_one_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_spans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-d20932a00182>\u001b[0m in \u001b[0;36mleave_one_out\u001b[0;34m(refs)\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m       \u001b[0mf1_ij\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrefs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mf1_ij\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mm_f1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-d20932a00182>\u001b[0m in \u001b[0;36mf1_score\u001b[0;34m(prediction, ground_truth)\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0mprediction_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize_answer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m   \u001b[0mground_truth_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize_answer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mground_truth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m   \u001b[0mcommon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction_tokens\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mground_truth_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-d20932a00182>\u001b[0m in \u001b[0;36mnormalize_answer\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mwhite_space_fix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremove_articles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremove_punc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-d20932a00182>\u001b[0m in \u001b[0;36mremove_articles\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mremove_articles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'\\b(a|an|the)\\b'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwhite_space_fix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/re.py\u001b[0m in \u001b[0;36msub\u001b[0;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[1;32m    208\u001b[0m     a replacement string to be used.\"\"\"\n\u001b[0;32m--> 209\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2098\u001b[0m                         \u001b[0;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2099\u001b[0;31m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2100\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2099\u001b[0m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2100\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2101\u001b[0;31m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0m\u001b[1;32m   2102\u001b[0m                                             value, tb, tb_offset=tb_offset)\n\u001b[1;32m   2103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[1;32m   1125\u001b[0m                                                                tb_offset)\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
          ]
        }
      ],
      "source": [
        "epochs = 3\n",
        "lr = 3e-5\n",
        "beta_1 = .9\n",
        "beta_2 = .999\n",
        "eps = 1e-6\n",
        "batch_size = 2\n",
        "accumulation_steps = 1\n",
        "accumulation_counter = 0\n",
        "\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "berthae = BertHAE(model, device).to(device)\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "loss_collection = []\n",
        "train_dataloader = DataLoader(current_file=0, current_index=0, batch_size=batch_size, shuffle=True, training=True)\n",
        "eval_dataloader = DataLoader(current_file=0, current_index=0, batch_size=1, shuffle=False, training=False)\n",
        "each_step_log = 100\n",
        "start_epoch = 0\n",
        "start_step = 0\n",
        "current_file = 0\n",
        "current_index = 0\n",
        "\n",
        "\n",
        "optimization_steps = int(epochs * len(train_dataloader) / accumulation_steps)\n",
        "warmup_ratio = .1\n",
        "warmup_steps = int(optimization_steps * warmup_ratio)\n",
        "\n",
        "optimizer = AdamW(berthae.parameters(), lr=lr, betas=(beta_1,beta_2), eps=eps)\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=warmup_steps,\n",
        "    num_training_steps=optimization_steps)\n",
        "\n",
        "# laod checkpoint if available\n",
        "if checkpoint_available:\n",
        "  print('loading checkpoint')\n",
        "  start_epoch, start_step, optimizer_dict, scheduler_dict, berthae_dict, current_file, current_index = load_checkpoint()\n",
        "  # load state dicts\n",
        "  berthae.load_state_dict(berthae_dict)\n",
        "  optimizer.load_state_dict(optimizer_dict)\n",
        "  scheduler.load_state_dict(scheduler_dict)\n",
        "\n",
        "current_file_index_ = current_file\n",
        "train_dataloader.reset_dataloader(current_file, current_index)\n",
        "berthae.train()\n",
        "for epoch in range(start_epoch, epochs):\n",
        "  train_step = 0\n",
        "  acc_loss = 0\n",
        "  log_step = 0\n",
        "\n",
        "  for data in train_dataloader:\n",
        "    if train_dataloader.data_manager.current_file_index != current_file_index_:\n",
        "      current_file_index_ = train_dataloader.data_manager.current_file_index\n",
        "      print(current_file_index_)\n",
        "      print('-------------')\n",
        "      if int(current_file_index_) % 3 == 0 and current_file_index_ != current_file:\n",
        "        save_checkpoint(epoch, current_file_index_, 0)\n",
        "    start_positions = data.pop('start_positions').to(device)\n",
        "    end_positions = data.pop('end_positions').to(device)\n",
        "    features = data.pop('features')\n",
        "    start_logits, end_logits = berthae(data)\n",
        "    loss = (loss_fn(start_logits, start_positions) + loss_fn(end_logits, end_positions)) / 2\n",
        "    loss = loss / accumulation_steps\n",
        "    acc_loss += loss.item()\n",
        "    loss.backward()\n",
        "\n",
        "    if (len(loss_collection) + 1) % each_step_log == 0:\n",
        "      print_loss(loss_collection, epoch, log_step + 1)\n",
        "      save_loss(loss_collection, epoch, log_step + 1)\n",
        "      loss_collection = []\n",
        "\n",
        "\n",
        "    accumulation_counter += 1\n",
        "    if accumulation_counter % accumulation_steps == 0:\n",
        "      loss_collection.append(acc_loss)\n",
        "      acc_loss = 0\n",
        "      log_step += 1\n",
        "      optimizer.step()\n",
        "      scheduler.step()\n",
        "      optimizer.zero_grad()\n",
        "      torch.cuda.empty_cache()\n",
        "      accumulation_counter = 0\n",
        "\n",
        "    train_step += 1\n",
        "\n",
        "  save_checkpoint(epoch + 1, 0, 0)\n",
        "  berthae.eval()\n",
        "  print('-------------------- Evaluation --------------------')\n",
        "  eval_p = EvalProcessOutput()\n",
        "  with torch.no_grad():\n",
        "    for step, data in enumerate(eval_dataloader):\n",
        "      start_positions = data.pop('start_positions')\n",
        "      end_positions = data.pop('end_positions')\n",
        "      features = data.pop('features')\n",
        "      start_logits, end_logits = berthae(data)\n",
        "      eval_p.process_feature_output(to_numpy(start_logits),\n",
        "                                    to_numpy(end_logits),\n",
        "                                    features)\n",
        "\n",
        "  eval_p.process_output()\n",
        "  filename = f'val_{epoch}.json'\n",
        "  write_to_file(eval_p.dialogs_answers, filename)\n",
        "  run_eval(filename, 'val_v0.2.json')\n",
        "  berthae.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDnsoudDYJz4"
      },
      "outputs": [],
      "source": [
        "data['input_ids'].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30ZqvcS2hfj3"
      },
      "outputs": [],
      "source": [
        "xlmr_tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n",
        "config = XLMRobertaConfig()\n",
        "config.vocab_size = xlmr_tokenizer.vocab_size  # setting both to have same vocab size"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "klvWCqdsSmEB"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}