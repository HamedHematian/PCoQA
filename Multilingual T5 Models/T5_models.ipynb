{"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n! pip install hazm\n! pip install accelerate\n! pip install bitsandbytes\n! pip install gdown\nfrom hazm import *\nimport gdown\nimport pickle as pk\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nimport torch\nimport json\nimport pickle\nimport shutil\nimport random\nimport unicodedata\nfrom tqdm import tqdm\nfrom copy import deepcopy\nimport os\nfrom collections import defaultdict, namedtuple\nimport transformers\nfrom transformers.optimization import get_linear_schedule_with_warmup\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import AdamW, Adam, RMSprop\nimport numpy as np\n\ndef make_dir(dir_name):\n  if not os.path.exists(dir_name):\n    os.mkdir(dir_name)\n\ndef read_file(filename):\n  with open(filename, 'r') as f:\n    return json.load(f)\n\ndef load_data(filename):\n  with open(filename, 'rb') as f:\n    x = pickle.load(f)\n  return x\n\ndef save_data(data, filename):\n    with open(filename, \"wb\") as f:\n        pickle.dump(data, f)\n\ndef send_2_device(tokens):\n  new_tokens = dict()\n  for k, v in tokens.items():\n    new_tokens[k] = v.to(device)\n  return new_tokens\n\ndef to_numpy(tensor):\n  return tensor.detach().cpu().numpy()\n\ndef get_prob(output):\n  scores = [score.view(-1) for score in output.scores]\n  sequences = output.sequences.view(-1)\n  log_probs = []\n  for score, seq in zip(scores, sequences[1: ]):\n    log_prob = F.log_softmax(score, dim=0)\n    log_probs.append(log_prob[seq])\n  log_probs = torch.stack(log_probs).mean().cpu().item()\n  return log_probs\n\ngdown.download(id=\"1tzaAHUkIkGzhbZpCIKmOYYQKvvqoRfsO\")\ngdown.download(id=\"16wPRHP2AC5WI2m7Y_fEWEOUYl7ynMKrb\")\ngdown.download(id=\"1qYU3601tCOI-MTQut8Nb7mSZMDLXuASY\")\n\nmake_dir('examples')\nmake_dir('features')\nmake_dir('examples/train')\nmake_dir('examples/eval')\nmake_dir('examples/test')\nmake_dir('features/train')\nmake_dir('features/eval')\nmake_dir('features/test')\n\n\ntrain_data = load_data('PCoQA_Train.pk')\neval_data = load_data('PCoQA_Eval.pk')\ntest_data = load_data('PCoQA_Test.pk')\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"id":"jRjo5wJxtqlN","execution":{"iopub.status.busy":"2024-07-08T13:15:17.886766Z","iopub.execute_input":"2024-07-08T13:15:17.887266Z","iopub.status.idle":"2024-07-08T13:16:45.026997Z","shell.execute_reply.started":"2024-07-08T13:15:17.887240Z","shell.execute_reply":"2024-07-08T13:16:45.026090Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"tMG4EprV8xyh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Eval Code","metadata":{"id":"ZdJaCucoguNK"}},{"cell_type":"code","source":"\"\"\"# Official Evaluation Code\"\"\"\n\nimport json, string, re\nfrom collections import Counter, defaultdict\n\n\ndef is_overlapping(x1, x2, y1, y2):\n  return max(x1, y1) <= min(x2, y2)\n\ndef normalize_answer(s):\n  \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n  def remove_articles(text):\n    return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n  def white_space_fix(text):\n    return ' '.join(text.split())\n  def remove_punc(text):\n    exclude = set(string.punctuation)\n    return ''.join(ch for ch in text if ch not in exclude)\n  def lower(text):\n    return text.lower()\n  return white_space_fix(remove_articles(remove_punc(lower(s))))\n\ndef f1_score(prediction, ground_truth):\n  prediction_tokens = normalize_answer(prediction).split()\n  ground_truth_tokens = normalize_answer(ground_truth).split()\n  common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n  num_same = sum(common.values())\n  if num_same == 0:\n    return 0\n  precision = 1.0 * num_same / len(prediction_tokens)\n  recall = 1.0 * num_same / len(ground_truth_tokens)\n  f1 = (2 * precision * recall) / (precision + recall)\n  return f1\n\ndef compute_span_overlap(pred_span, gt_span, text):\n  if gt_span == 'غیرقابل‌پاسخ':\n    if pred_span == 'غیرقابل‌پاسخ':\n      return 'Exact match', 1.0\n    return 'No overlap', 0.\n  fscore = f1_score(pred_span, gt_span)\n  pred_start = text.find(pred_span)\n  gt_start = text.find(gt_span)\n\n  if pred_start == -1 or gt_start == -1:\n    return 'Span indexing error', fscore\n\n  pred_end = pred_start + len(pred_span)\n  gt_end = gt_start + len(gt_span)\n\n  fscore = f1_score(pred_span, gt_span)\n  overlap = is_overlapping(pred_start, pred_end, gt_start, gt_end)\n\n  if exact_match_score(pred_span, gt_span):\n    return 'Exact match', fscore\n  if overlap:\n    return 'Partial overlap', fscore\n  else:\n    return 'No overlap', fscore\n\ndef exact_match_score(prediction, ground_truth):\n  return (normalize_answer(prediction) == normalize_answer(ground_truth))\n\ndef display_counter(title, c, c2=None):\n  print(title)\n  for key, _ in c.most_common():\n    if c2:\n      print('%s: %d / %d, %.1f%%, F1: %.1f' % (\n        key, c[key], sum(c.values()), c[key] * 100. / sum(c.values()), sum(c2[key]) * 100. / len(c2[key])))\n    else:\n      print('%s: %d / %d, %.1f%%' % (key, c[key], sum(c.values()), c[key] * 100. / sum(c.values())))\n\ndef leave_one_out_max(prediction, ground_truths, article):\n  if len(ground_truths) == 1:\n    return metric_max_over_ground_truths(prediction, ground_truths, article)[1]\n  else:\n    t_f1 = []\n    # leave out one ref every time\n    for i in range(len(ground_truths)):\n      idxes = list(range(len(ground_truths)))\n      idxes.pop(i)\n      refs = [ground_truths[z] for z in idxes]\n      t_f1.append(metric_max_over_ground_truths(prediction, refs, article)[1])\n  return 1.0 * sum(t_f1) / len(t_f1)\n\n\ndef metric_max_over_ground_truths(prediction, ground_truths, article):\n  scores_for_ground_truths = []\n  for ground_truth in ground_truths:\n    score = compute_span_overlap(prediction, ground_truth, article)\n    scores_for_ground_truths.append(score)\n  return max(scores_for_ground_truths, key=lambda x: x[1])\n\n\ndef handle_cannot(refs):\n  num_cannot = 0\n  num_spans = 0\n  for ref in refs:\n    if ref == 'غیرقابل‌پاسخ':\n      num_cannot += 1\n    else:\n      num_spans += 1\n  if num_cannot >= num_spans:\n    refs = ['CANNOTANSWER']\n  else:\n    refs = [x for x in refs if x != 'غیرقابل‌پاسخ']\n  return refs\n\n\ndef leave_one_out(refs):\n  if len(refs) == 1:\n    return 1.\n  splits = []\n  for r in refs:\n    splits.append(r.split())\n  t_f1 = 0.0\n  for i in range(len(refs)):\n    m_f1 = 0\n    for j in range(len(refs)):\n      if i == j:\n        continue\n      f1_ij = f1_score(refs[i], refs[j])\n      if f1_ij > m_f1:\n        m_f1 = f1_ij\n    t_f1 += m_f1\n  return t_f1 / len(refs)\n\n\n\n\n\n\ndef eval_fn(val_results, model_results, verbose):\n  span_overlap_stats = Counter()\n  sentence_overlap = 0.\n  para_overlap = 0.\n  total_qs = 0.\n  f1_stats = defaultdict(list)\n  unfiltered_f1s = []\n  total_dials = 0.\n  unanswerables = []\n  for p in val_results:\n    for par in p['paragraphs']:\n      did = par['id']\n      qa_list = par['qas']\n      good_dial = 1.\n      for qa in qa_list:\n        q_idx = qa['id']\n        val_spans = [anss['text'] for anss in qa['answers']]\n        val_spans = handle_cannot(val_spans)\n        hf1 = leave_one_out(val_spans)\n\n        if did not in model_results or q_idx not in model_results[did]:\n          # print(did, q_idx, 'no prediction for this dialogue id')\n          good_dial = 0\n          f1_stats['NO ANSWER'].append(0.0)\n          if val_spans == ['غیرقابل‌پاسخ']:\n            unanswerables.append(0.0)\n          total_qs += 1\n          unfiltered_f1s.append(0.0)\n          if hf1 >= .4:\n            human_f1.append(hf1)\n          continue\n\n        pred_span, pred_yesno, pred_followup = model_results[did][q_idx]\n\n        max_overlap, _ = metric_max_over_ground_truths( \\\n          pred_span, val_spans, par['context'])\n        max_f1 = leave_one_out_max( \\\n          pred_span, val_spans, par['context'])\n        unfiltered_f1s.append(max_f1)\n\n        # dont eval on low agreement instances\n        if hf1 < .4:\n          continue\n\n        human_f1.append(hf1)\n\n        if val_spans == ['غیرقابل‌پاسخ']:\n          unanswerables.append(max_f1)\n        if verbose:\n          print(\"-\" * 20)\n          print(pred_span)\n          print(val_spans)\n          print(max_f1)\n          print(\"-\" * 20)\n        if max_f1 >= hf1:\n          HEQ += 1.\n        else:\n          good_dial = 0.\n        span_overlap_stats[max_overlap] += 1\n        f1_stats[max_overlap].append(max_f1)\n        total_qs += 1.\n      DHEQ += good_dial\n      total_dials += 1\n\n\n  DHEQ_score = 100.0 * DHEQ / total_dials\n  HEQ_score = 100.0 * HEQ / total_qs\n  all_f1s = sum(f1_stats.values(), [])\n  overall_f1 = 100.0 * sum(all_f1s) / len(all_f1s)\n  unfiltered_f1 = 100.0 * sum(unfiltered_f1s) / len(unfiltered_f1s)\n  unanswerable_score = (100.0 * sum(unanswerables) / len(unanswerables))\n  metric_json = {\"unfiltered_f1\": unfiltered_f1, \"f1\": overall_f1, \"HEQ\": HEQ_score, \"DHEQ\": DHEQ_score, \"yes/no\": yesno_score, \"followup\": followup_score, \"unanswerable_acc\": unanswerable_score}\n  if verbose:\n    print(\"=======================\")\n    display_counter('Overlap Stats', span_overlap_stats, f1_stats)\n  print(\"=======================\")\n  print('Overall F1: %.1f' % overall_f1)\n  with open('val_report.txt', 'a') as f:\n    f.write('Overall F1: %.1f' % overall_f1)\n\n  print('Unfiltered F1 ({0:d} questions): {1:.1f}'.format(len(unfiltered_f1s), unfiltered_f1))\n  print('Accuracy On Unanswerable Questions: {0:.1f} %% ({1:d} questions)'.format(unanswerable_score, len(unanswerables)))\n  print('Human F1: %.1f' % (100.0 * sum(human_f1) / len(human_f1)))\n  print('Model F1 >= Human F1 (Questions): %d / %d, %.1f%%' % (HEQ, total_qs, 100.0 * HEQ / total_qs))\n  print('Model F1 >= Human F1 (Dialogs): %d / %d, %.1f%%' % (DHEQ, total_dials, 100.0 * DHEQ / total_dials))\n  print(\"=======================\")\n  output_string = 'Overall F1: %.1f\\n' % overall_f1\n  output_string += 'Yes/No Accuracy : %.1f\\n' % yesno_score\n  output_string += 'Followup Accuracy : %.1f\\n' % followup_score\n  output_string += 'Unfiltered F1 ({0:d} questions): {1:.1f}\\n'.format(len(unfiltered_f1s), unfiltered_f1)\n  output_string += 'Accuracy On Unanswerable Questions: {0:.1f} %% ({1:d} questions)\\n'.format(unanswerable_score, len(unanswerables))\n  output_string += 'Human F1: %.1f\\n' % (100.0 * sum(human_f1) / len(human_f1))\n  output_string += 'Model F1 >= Human F1 (Questions): %d / %d, %.1f%%\\n' % (HEQ, total_qs, 100.0 * HEQ / total_qs)\n  output_string += 'Model F1 >= Human F1 (Dialogs): %d / %d, %.1f%%' % (DHEQ, total_dials, 100.0 * DHEQ / total_dials)\n\n  # save_prediction(epoch, train_step, output_string)\n\n  return metric_json\n\ndef run_eval(mode, predicted_obj):\n  new_eval_data = dict()\n  mean_f1s_ = [[] for _ in range(30)]\n  if mode == 'eval':\n    main_data = eval_data\n  elif mode == 'test':\n    main_data = test_data\n\n  for data in main_data:\n    new_eval_data[str(data['id'])] = data\n\n\n  res = {\n      'question': [],\n      'pred': [],\n      'orig': [],\n      'f1': [],\n      'heq-q': []\n  }\n\n  f1s = []\n  dialog_f1s = []\n  dialog_hfs = []\n  unans_score = []\n  heq_q = []\n  heq_d = []\n  heq_m = []\n  EM = []\n\n  dialogs_f1s = dict()\n  dialogs_hfs = dict()\n\n  results = []\n  for q_idx, model_answer in predicted_obj.answers.items():\n    d_id, q_num = q_idx.split('#')[0], int(q_idx.split('#')[1])\n    d_id, q_num = q_idx.split('#')[0], int(q_idx.split('#')[1])\n\n    if d_id not in dialogs_f1s.keys():\n      dialogs_f1s[d_id] = []\n      dialogs_hfs[d_id] = []\n\n    qa = new_eval_data[d_id]['qas'][q_num]\n    answers_num = len(qa['answers'])\n    orig_answers = [qa['answers'][qidx]['text'] for qidx in range(answers_num)]\n    if 'غیرقابل‌پاسخ' in orig_answers:\n      if model_answer.startswith('غیرقابل'):\n        unans_score.append(1.0)\n      else:\n        unans_score.append(0.0)\n\n    res['question'].append(qa['question'])\n    res['pred'].append(model_answer)\n    res['orig'].append(orig_answers)\n\n    context = new_eval_data[d_id]['article']\n    hf = qa['hf']\n    f1s_ = [compute_span_overlap(model_answer, orig_answer, context)[1] for orig_answer in orig_answers]\n    max_f1 = max(f1s_)\n    dialog_hfs.append(hf)\n    dialog_f1s.append(max_f1)\n    f1s.append(max_f1)\n    mean_f1s_[q_num].append(max_f1)\n\n    if int(max_f1) == 1:\n      EM.append(1.)\n    else:\n      EM.append(0.)\n\n    if max_f1 >= hf:\n      heq_q.append(1)\n    else:\n      heq_q.append(0)\n\n    res['f1'].append(max_f1)\n    res['heq-q'].append(heq_q)\n\n    dialogs_f1s[d_id].append(max_f1)\n    dialogs_hfs[d_id].append(hf)\n\n\n  for key in dialogs_f1s.keys():\n    dialog_f1s = dialogs_f1s[key]\n    dialog_hfs = dialogs_hfs[key]\n\n    heq_d.append(all(x >= y for x, y in zip(dialog_f1s, dialog_hfs)))\n    heq_m.append(sum(dialog_f1s) >= sum(dialog_hfs))\n\n\n\n  f1_score_ = sum(f1s) / len(f1s)\n  heq_q_score_ = sum(heq_q) / len(heq_q)\n  heq_m_score_ = sum(heq_m) / len(heq_m)\n  heq_d_score_ = sum(heq_d) / len(heq_d)\n  unans_score_ = sum(unans_score) / len(unans_score)\n  EM_score_ = sum(EM) / len(EM)\n\n\n  mean_f1s = [sum(mean_f1) / (1e-8 + len(mean_f1)) for mean_f1 in mean_f1s_]\n  print(f'f1 score is {f1_score_}')\n  print(f'HEQ-Q score is {heq_q_score_}')\n  print(f'HEQ-M score is {heq_m_score_}')\n  print(f'HEQ-D score is {heq_d_score_}')\n  print(f'EM score is {EM_score_}')\n  print(f'Unanswerable score is {unans_score_}')\n  print(mean_f1s)\n  return f1_score_, heq_q_score_, heq_d_score_, mean_f1s, res","metadata":{"id":"QeJpAKjcfb7c","execution":{"iopub.status.busy":"2024-07-08T13:16:45.028761Z","iopub.execute_input":"2024-07-08T13:16:45.029248Z","iopub.status.idle":"2024-07-08T13:16:45.082936Z","shell.execute_reply.started":"2024-07-08T13:16:45.029223Z","shell.execute_reply":"2024-07-08T13:16:45.081964Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# MT5\n\n","metadata":{"id":"ut2Aj_6tH2da"}},{"cell_type":"code","source":"# %%capture\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\ntokenizer = AutoTokenizer.from_pretrained(\"google/mt5-base\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"google/mt5-base\")","metadata":{"id":"kgXtyk3GRqJn","execution":{"iopub.status.busy":"2024-07-08T13:16:45.083983Z","iopub.execute_input":"2024-07-08T13:16:45.084305Z","iopub.status.idle":"2024-07-08T13:16:45.095390Z","shell.execute_reply.started":"2024-07-08T13:16:45.084275Z","shell.execute_reply":"2024-07-08T13:16:45.094569Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# MT0","metadata":{"id":"EBg2iFwXYcSP"}},{"cell_type":"code","source":"%%capture\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\ntokenizer = AutoTokenizer.from_pretrained(\"bigscience/mt0-base\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"bigscience/mt0-base\")","metadata":{"id":"2G4ztsJ7RquY","execution":{"iopub.status.busy":"2024-07-08T13:16:45.097544Z","iopub.execute_input":"2024-07-08T13:16:45.097940Z","iopub.status.idle":"2024-07-08T13:20:12.478302Z","shell.execute_reply.started":"2024-07-08T13:16:45.097916Z","shell.execute_reply":"2024-07-08T13:20:12.477424Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Rest","metadata":{"id":"fWVdUpVbZU6G"}},{"cell_type":"code","source":"class CQA_DATA:\n\n  def __init__(self,\n               question,\n               context,\n               history,\n               answer,\n               qid,\n               q_num,\n               answer_start,\n               answer_end,\n               is_answerable):\n\n    self.question = question\n    self.context = context\n    self.answer = answer\n    self.history = history\n    self.qid = qid\n    self.q_num = q_num\n    self.answer_start = answer_start\n    self.answer_end = answer_end\n    self.is_answerable = is_answerable\n    self.cleaned_context = None\n    self.cleaned_answer = {\n        'text': self.answer,\n        'start': self.answer_start,\n        'end': self.answer_end\n    }\n    self.cleaned_context = context\n\n  def __repr__(self):\n    repr = ''\n    repr += 'context -> ' + self.context[:100] + '\\n'\n    repr += 'question ->' + self.question + '\\n'\n    repr += 'question id ->' + str(self.qid) + '\\n'\n    repr += 'turn_number ->' + str(self.q_num) + '\\n'\n    repr += 'answer ->' + self.answer + '\\n'\n    return repr\n\nclass Feature:\n\n  def __init__(self,\n               qid,\n               question_part,\n               input_ids,\n               attention_mask,\n               feature_answerability,\n               offset_mappings,\n               max_context_dict,\n               label_tokens,\n               is_answerable,\n               context,\n               cleaned_context,\n               context_start,\n               context_end,\n               example_start_char,\n               example_end_char,\n               example_answer):\n\n    self.qid = qid\n    self.question_part = question_part\n    self.input_ids = input_ids\n    self.attention_mask = attention_mask\n    self.feature_answerability = feature_answerability\n    self.offset_mappings = offset_mappings\n    self.max_context_dict = max_context_dict\n    self.label_tokens = label_tokens\n    self.is_answerable = is_answerable\n    self.context = context\n    self.cleaned_context = cleaned_context\n    self.context_start = context_start\n    self.context_end = context_end\n    self.example_start_char = example_start_char\n    self.example_end_char = example_end_char\n    self.example_answer = example_answer\n\n  def __repr__(self):\n    repr = ''\n    repr += 'qid --> ' + str(self.qid) + '\\n'\n    repr += 'quesion part --> ' + str(self.question_part) + '\\n'\n    repr += 'answer --> ' + self.example_answer + '\\n'\n    return repr\n\n\"\"\"# Examples\"\"\"\n\ndef make_examples(data, data_type, num_sample, clean_samples=True):\n  examples = []\n  each_file_size = 1000\n  example_file_index = 0\n  data_dir = f'examples/{data_type}/'\n\n  for dialog_num, dialog in enumerate(tqdm(data[:num_sample ], leave=False, position=0)):\n    dialog_history = []\n    dialog_container = []\n    dialog_id = dialog['id']\n    title = dialog['title']\n    context = dialog['article']\n    dialog_len = len(dialog['qas'])\n\n    for q_num, qa in enumerate(dialog['qas']):\n      history = []\n      question = qa['rewritten_question']\n      answer = qa['answers'][0]\n      is_answerable = False if answer['text'] == 'غیرقابل‌پاسخ' else True\n\n      if not is_answerable:\n        answer_start = 0\n        answer_end = 0\n\n      if not q_num == 0:\n        history = deepcopy(dialog_history)\n\n      qid = f'{dialog_id}#{q_num}'\n      cqa_example = CQA_DATA(question=question,\n                             context=context[ :-13],\n                             history=history,\n                             answer=answer['text'],\n                             qid=qid,\n                             q_num=q_num,\n                             answer_start=answer['start'],\n                             answer_end=answer['end'],\n                             is_answerable=is_answerable)\n\n      examples.append(cqa_example)\n      dialog_history.append([cqa_example.question, cqa_example.answer])\n\n    if (dialog_num + 1) % each_file_size == 0:\n      filename = f'{data_type}_examples_' + str(example_file_index) + '.bin'\n      save_data(examples, os.path.join(data_dir, filename))\n      example_file_index += 1\n      examples = []\n\n  if examples != []:\n    filename = f'{data_type}_examples_' + str(example_file_index) + '.bin'\n    save_data(examples, os.path.join(data_dir, filename))","metadata":{"id":"YeVLZD25vsl_","execution":{"iopub.status.busy":"2024-07-08T13:20:12.479409Z","iopub.execute_input":"2024-07-08T13:20:12.479720Z","iopub.status.idle":"2024-07-08T13:20:12.501125Z","shell.execute_reply.started":"2024-07-08T13:20:12.479694Z","shell.execute_reply":"2024-07-08T13:20:12.500262Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def make_features(data_type):\n  data_dir = f'examples/{data_type}/'\n  example_files = os.listdir(data_dir)\n  example_files = [os.path.join(data_dir, example_file) for example_file in example_files]\n  features_list = []\n  features_dir = f'features/{data_type}/'\n  current_file_index = 0\n  max_history_to_consider = 5\n  offset_ = 1\n\n  for file_index, filename in enumerate(example_files):\n    examples = load_data(filename)\n    for example in examples:\n      example_features = []\n      concatenated_question = []\n\n      # concat history\n      for hist in example.history[-max_history_to_consider:]:\n        concatenated_question.append(hist[0])\n\n      # append current question to concatenated question\n      concatenated_question.append(example.question)\n\n      # make string out of concatenated question\n      concatenated_question = ' '.join(concatenated_question)\n\n      text_tokens = tokenizer(\n          concatenated_question,\n          example.cleaned_context,\n          max_length=512,\n          padding='max_length',\n          truncation='only_second',\n          return_overflowing_tokens=True,\n          return_offsets_mapping=True,\n          stride=128)\n\n\n\n      # find start and end of context\n      for idx in range(len(text_tokens['input_ids'])):\n        found_start = False\n        found_end = False\n        context_start = 0\n        context_end = 511\n        max_context_dict = {}\n\n        for token_idx, token in enumerate(text_tokens['offset_mapping'][idx]):\n          if token[0] == 0 and token[1] == 0:\n            context_start = token_idx + offset_\n            break\n\n        for token_idx, token in enumerate(text_tokens['offset_mapping'][idx][context_start:]):\n          if token[0] == 0 and token[1] == 0:\n            context_end = token_idx + context_start - 1\n            break\n\n        chunk_offset_mapping = text_tokens['offset_mapping'][idx]\n        for context_idx, data in enumerate(chunk_offset_mapping[context_start: context_end + 1]):\n          max_context_dict[f'({data[0]},{data[1]})'] = min(context_idx, context_end - context_idx) + (context_end - context_start + 1) * .01\n\n        # find and mark current question answer\n        last_token = None\n        for token_idx, token in enumerate(chunk_offset_mapping[context_start: context_end + 1]):\n          if token[0] == example.cleaned_answer['start'] and not found_start:\n            found_start = True\n            start = token_idx + context_start\n          elif last_token and last_token[0] < example.cleaned_answer['start'] and token[0] > example.cleaned_answer['start']:\n            found_start = True\n            start = (token_idx - 1) + context_start\n          if token[1] == example.cleaned_answer['end'] and not found_end:\n            found_end = True\n            end = token_idx + context_start\n          elif last_token and last_token[1] < example.cleaned_answer['end'] and token[1] > example.cleaned_answer['end'] and last_token:\n            found_end = True\n            end = token_idx + context_start\n          last_token = token\n\n\n        # add feature to features list\n        if end < start and found_start and found_end:\n          assert False, 'start and end do not match'\n\n        elif ((not found_start) or (not found_end)) and data_type != 'train':\n          start, end = 0, 0\n\n        if (not found_start) or (not found_end):\n          answer = 'غیرقابل‌پاسخ'\n        else:\n          answer = example.answer\n\n        feature_answerability = False if answer == 'غیرقابل‌پاسخ' else True\n\n        label_tokens = tokenizer(\n          example.answer,\n          truncation=True)['input_ids']\n\n        feature_answerability = torch.LongTensor([feature_answerability])\n\n        # print(answerability)\n        # print(answer)\n        # print('-----------')\n\n        example_features.append(Feature(example.qid,\n                                          idx,\n                                          text_tokens['input_ids'][idx],\n                                          text_tokens['attention_mask'][idx],\n                                          feature_answerability,\n                                          text_tokens['offset_mapping'][idx],\n                                          max_context_dict,\n                                          label_tokens,\n                                          example.is_answerable,\n                                          example.context,\n                                          example.cleaned_context,\n                                          context_start,\n                                          context_end,\n                                          example.answer_start,\n                                          example.answer_end,\n                                          example.answer))\n      features_list.extend(example_features)\n  filename = f'{data_type}_features_' + str(file_index) + '.bin'\n  print(len(features_list))\n  save_data(features_list, os.path.join(features_dir, filename))\n\nmake_examples(train_data, 'train', 1000000)\nmake_examples(eval_data, 'eval', 1000000)\nmake_examples(test_data, 'test', 1000000)\nmake_features('train')\nmake_features('eval')\nmake_features('test')","metadata":{"id":"xupZBZDZC_Mr","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d9c9a4a5-cd0d-4825-af0d-81e6ed9b5df9","execution":{"iopub.status.busy":"2024-07-08T13:20:12.502372Z","iopub.execute_input":"2024-07-08T13:20:12.502693Z","iopub.status.idle":"2024-07-08T13:21:26.199425Z","shell.execute_reply.started":"2024-07-08T13:20:12.502663Z","shell.execute_reply":"2024-07-08T13:21:26.198430Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"                                                   \r","output_type":"stream"},{"name":"stdout","text":"18066\n3675\n3687\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"id":"yBTAT7CCVQgI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DataManager:\n\n  def __init__(self, current_file, current_index, data_dir, batch_size, shuffle=True):\n    self.files = sorted(os.listdir(data_dir), key=lambda x: int(x.split('_')[2].split('.')[0]))\n    self.files = list(map(lambda x: os.path.join(data_dir, x), self.files))\n    self.shuffle = shuffle\n    self.data_len = 0\n    for filename in self.files:\n      self.data_len += len(load_data(filename))\n    self.batch_size = batch_size\n    self.reset_datamanager(current_file, current_index)\n\n  def reset_datamanager(self, current_file_index, current_index):\n    self.current_index = current_index\n    self.current_file_index = current_file_index\n    self.features = self.load_data_file(self.files[self.current_file_index])\n\n  def load_data_file(self, filename):\n    if self.shuffle:\n      data = load_data(filename)\n      random.shuffle(data)\n      return data\n    else:\n      return load_data(filename)\n\n  def next(self):\n    temp = self.features[self.current_index:self.current_index + self.batch_size]\n    self.temp = temp\n    self.current_index += self.batch_size\n    if self.current_index >= len(self.features):\n      self.current_index = 0\n      self.current_file_index += 1\n      if self.current_file_index == len(self.files):\n        self.reset_datamanager(current_file_index=0, current_index=0)\n        return temp, True\n      else:\n        self.features = self.load_data_file(self.files[self.current_file_index])\n    return temp, False\n\n\nclass DataLoader:\n\n  def __init__(self, current_file, current_index, batch_size, shuffle=True, data_type='train'):\n    self.data_type = data_type\n    self.batch_size = batch_size\n    self.data_manager = DataManager(current_file, current_index, f'features/{data_type}/', batch_size, shuffle)\n\n  def __iter__(self):\n    self.stop_iteration = False\n    return self\n\n  def __len__(self):\n    return int(self.data_manager.data_len // self.batch_size)\n\n  def reset_dataloader(self, current_file, current_index):\n    self.data_manager.reset_datamanager(current_file, current_index)\n\n  def features_2_tensor(self, features):\n    x = dict()\n    x['input_ids'] = torch.LongTensor([feature.input_ids for feature in features])\n    x['attention_mask'] = torch.LongTensor([feature.attention_mask for feature in features])\n    x['label_tokens'] = torch.LongTensor([feature.label_tokens for feature in features])\n    x['feature_answerability'] = torch.LongTensor([feature.feature_answerability for feature in features])\n    x['features'] = features\n    return x\n\n  def __next__(self):\n    if self.stop_iteration:\n      raise StopIteration\n    features, self.stop_iteration = self.data_manager.next()\n    return self.features_2_tensor(features)","metadata":{"id":"UYkrFnd0a6F0","execution":{"iopub.status.busy":"2024-07-08T13:21:26.202425Z","iopub.execute_input":"2024-07-08T13:21:26.202728Z","iopub.status.idle":"2024-07-08T13:21:26.219442Z","shell.execute_reply.started":"2024-07-08T13:21:26.202704Z","shell.execute_reply":"2024-07-08T13:21:26.218661Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"\"\"\"# Saving Settings\"\"\"\nhist_num_str = 11\n\ndrive_prefix = f'.'\ndrive_checkpoint_dir = 'Checkpoint/'\ndrive_log_dir = 'Log/'\ncheckpoint_dir = os.path.join(drive_prefix, drive_checkpoint_dir)\nlog_dir = os.path.join(drive_prefix, drive_log_dir)\n\nmeta_log_file = os.path.join(drive_prefix, drive_log_dir, 'test.txt')\nprediction_file_prefix = os.path.join(drive_prefix, drive_log_dir, 'prediction_')\nloss_log_file = os.path.join(drive_prefix, drive_log_dir, 'loss.txt')\nmean_f1_file = os.path.join(drive_prefix, drive_log_dir, 'mean_f1.txt')\neval_res_file = os.path.join(drive_prefix, drive_log_dir, 'eval_result.json')\ntest_res_file = os.path.join(drive_prefix, drive_log_dir, 'test_result.json')\n\nif not os.path.exists(drive_prefix):\n  os.mkdir(drive_prefix)\n  print('crated saved dir')\nif not os.path.exists(checkpoint_dir):\n  os.mkdir(checkpoint_dir)\nif not os.path.exists(log_dir):\n  os.mkdir(log_dir)\n\nwith open(meta_log_file, 'w') as f:\n  pass\n# check if drive is accessible\ntry:\n   with open(os.path.join(drive_prefix, drive_log_dir, 'test.txt'), 'r') as f:\n      pass\nexcept:\n  print('No Access to Drive')\n  exit()\n\n\ndef print_loss(loss_collection, epoch, step):\n  txt = f'EPOCH [{epoch + 1}/{epochs}] | STEP [{step}/{int(len(train_dataloader) / accumulation_steps)}] | Loss {round(sum(loss_collection) / len(loss_collection), 4)}'\n  print(txt)\n\ndef save_loss(loss_collection, epoch, step):\n  txt = f'EPOCH [{epoch + 1}/{epochs}] | STEP [{step}/{int(len(train_dataloader) / accumulation_steps)}] | Loss {round(sum(loss_collection) / len(loss_collection), 4)}'\n  with open(loss_log_file, 'a') as f:\n    f.write(txt)\n    f.write('\\n')\n\n\n\ncheckpoint_available = False\n\n\ndef save_prediction(epoch, step, prediction_log):\n  with open(os.path.join(drive_prefix, drive_log_dir, 'prediction.txt'), 'a') as f:\n    f.write(f'--------- EPOCH {epoch} STEP {step} ---------\\n')\n    f.write(prediction_log)\n    f.write('\\n')\n    f.write('\\n')\n\ndef save_checkpoint(filename):\n  checkpoint_config = {\n  'model_dict': model_p.state_dict()}\n  torch.save(checkpoint_config, filename)\n\ndef load_checkpoint(current_checkpoint):\n    checkpoint_config = torch.load(current_checkpoint)\n    return checkpoint_config['model_dict']","metadata":{"id":"QMXfa4etiVB1","execution":{"iopub.status.busy":"2024-07-08T13:21:26.220770Z","iopub.execute_input":"2024-07-08T13:21:26.221005Z","iopub.status.idle":"2024-07-08T13:21:26.236146Z","shell.execute_reply.started":"2024-07-08T13:21:26.220985Z","shell.execute_reply":"2024-07-08T13:21:26.235420Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class EvalProcessOutput:\n  def __init__(self):\n    self.features = dict()\n    self.dialog_answers = defaultdict(list)\n    self.answers = dict()\n    self.dialog_len = dict()\n    self.log = dict()\n\n  def process_feature_output(self, feature, predicted_text, prob):\n    qid = feature.qid\n    q_num = int(qid.split('#')[1])\n    dialog_id = qid.split('#')[0]\n\n    if dialog_id not in self.dialog_len.keys():\n      self.dialog_len[dialog_id] = 0\n\n    if qid not in self.features.keys():\n      self.features[qid] = []\n      self.dialog_len[dialog_id] += 1\n    self.features[qid].append([predicted_text, prob, feature.example_answer])\n\n  def process_output(self):\n    for dialog_id, dialog_len in self.dialog_len.items():\n      self.dialog_answers[dialog_id] = ['' for _ in range(dialog_len)]\n\n    for qid, text_prob in self.features.items():\n      sorted_preds = sorted(text_prob, key=lambda x: x[1], reverse=True)\n      best_pred = sorted_preds[0]\n      q_num = int(qid.split('#')[1])\n      dialog_id = qid.split('#')[0]\n      self.dialog_answers[dialog_id][q_num] = best_pred[0]\n      self.log[qid] = best_pred\n      if best_pred[1] < .2:\n        self.answers[qid] = 'غیرقابل‌پاسخ'\n      else:\n        self.answers[qid] = best_pred[0]","metadata":{"id":"w1Rr3_tCg_cu","execution":{"iopub.status.busy":"2024-07-08T13:21:26.237258Z","iopub.execute_input":"2024-07-08T13:21:26.237517Z","iopub.status.idle":"2024-07-08T13:21:26.248318Z","shell.execute_reply.started":"2024-07-08T13:21:26.237495Z","shell.execute_reply":"2024-07-08T13:21:26.247506Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class Persian_Model(nn.Module):\n\n  def __init__(self, transformer):\n    super(Persian_Model, self).__init__()\n    self.transformer = transformer\n    self.answerability_head = nn.Linear(self.transformer.config.hidden_size, 2)\n    self.answerability_loss_fn = nn.CrossEntropyLoss(weight=torch.tensor([.5, 1.]).to(device))\n    nn.init.normal_(self.answerability_head.weight, mean=.0, std=.02)\n\n\n  def forward(self, input_ids, attention_mask, label_tokens, feature_answerability, mode):\n\n    input_ids = input_ids.view(1, -1)\n    attention_mask = attention_mask.view(1, -1)\n    label_tokens = label_tokens.view(1, -1)\n    feature_answerability = feature_answerability.view(-1)\n\n    if mode == 'train':\n      output = self.transformer(input_ids=input_ids,\n                                attention_mask=attention_mask,\n                                labels=label_tokens,\n                                output_hidden_states=True)\n      s_token_repr = output.encoder_hidden_states[-1][:, 0, :]\n      answerability_score = self.answerability_head(s_token_repr)\n      answerability_loss = self.answerability_loss_fn(answerability_score, feature_answerability)\n      if feature_answerability:\n        loss = output.loss + .5 * answerability_loss\n      else:\n        loss = answerability_loss\n      return loss\n\n    if mode == 'eval':\n      output = model.generate(input_ids=input_ids.view(1, -1),\n                              attention_mask=attention_mask.view(1, -1),\n                              max_length=150,\n                              do_sample=True,\n                              top_p=.2,\n                              top_k=100,\n                              temperature=.7,\n                              return_dict_in_generate=True,\n                              output_hidden_states=True)\n      s_token_repr = output.encoder_hidden_states[-1][:, 0, :]\n      answerability_score = self.answerability_head(s_token_repr)\n      return output, torch.softmax(answerability_score.view(-1), dim=0)[1].cpu().item()\n\n","metadata":{"id":"4G-6MFrk-Cvd","execution":{"iopub.status.busy":"2024-07-08T13:21:26.251495Z","iopub.execute_input":"2024-07-08T13:21:26.251776Z","iopub.status.idle":"2024-07-08T13:21:26.263093Z","shell.execute_reply.started":"2024-07-08T13:21:26.251753Z","shell.execute_reply":"2024-07-08T13:21:26.262260Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"epochs = 3\nlr = 4e-5\nbeta_1 = .9\nbeta_2 = .999\neps = 1e-6\nbatch_size = 1\nweight_decay = 0.0\naccumulation_steps = 10\naccumulation_counter = 0\nq_scores = dict()\nh_f1 = 0\nk = 3\nweight_decay = 0.0\nf1_list = []\nbest_eval_f1 = 0.0\n\ndevice = 'cuda:0' if torch.cuda.is_available() else 'cpu'\nmodel_p = Persian_Model(model).to(device)\nloss_fn = nn.CrossEntropyLoss().to(device)\n\nloss_collection = []\ntrain_dataloader = DataLoader(current_file=0, current_index=0, batch_size=batch_size, shuffle=True, data_type='train')\neval_dataloader = DataLoader(current_file=0, current_index=0, batch_size=1, shuffle=False, data_type='eval')\ntest_dataloader = DataLoader(current_file=0, current_index=0, batch_size=1, shuffle=False, data_type='test')\neach_step_log = 1000\nstart_epoch = 0\nstart_step = 0\ncurrent_file = 0\ncurrent_index = 0\n\n\noptimization_steps = int(epochs * len(train_dataloader) / accumulation_steps)\nwarmup_ratio = .1\nwarmup_steps = int(optimization_steps * warmup_ratio)\n\noptimizer = AdamW(model_p.parameters(), lr=lr, betas=(beta_1,beta_2), eps=eps, weight_decay=weight_decay)\nscheduler = get_linear_schedule_with_warmup(\n    optimizer=optimizer,\n    num_warmup_steps=warmup_steps,\n    num_training_steps=optimization_steps)\n\n# laod checkpoint if available\nif checkpoint_available:\n  print('loading checkpoint')\n  start_epoch, start_step, optimizer_dict, scheduler_dict, qa_model_dict, current_file, current_index = load_checkpoint()\n  # load state dicts\n  model_p.load_state_dict(qa_model_dict)\n  optimizer.load_state_dict(optimizer_dict)\n  scheduler.load_state_dict(scheduler_dict)\n\ncurrent_file_index_ = current_file\ntrain_dataloader.reset_dataloader(current_file, current_index)\nmodel_p.train()\n\n\nfor epoch in range(start_epoch, epochs):\n  train_step = 1\n  acc_loss = 0\n  log_step = 0\n\n  for data in train_dataloader:\n    input_ids = data.pop('input_ids').to(device)\n    attention_mask = data.pop('attention_mask').to(device)\n    label_tokens = data.pop('label_tokens').to(device)\n    feature_answerability = data.pop('feature_answerability').to(device)\n    features = data.pop('features')\n\n    # run output\n    loss = model_p(input_ids=input_ids,\n                     attention_mask=attention_mask,\n                     label_tokens=label_tokens,\n                     feature_answerability=feature_answerability,\n                     mode='train')\n    \n    loss /= accumulation_steps\n\n    acc_loss += loss.item()\n    loss.backward()\n\n\n    if train_step % each_step_log == 0:\n      print_loss(loss_collection, epoch, log_step + 1)\n      save_loss(loss_collection, epoch, log_step + 1)\n      loss_collection = []\n\n    accumulation_counter += 1\n    if accumulation_counter % accumulation_steps == 0:\n      loss_collection.append(acc_loss)\n      acc_loss = 0\n      log_step += 1\n      optimizer.step()\n      scheduler.step()\n      optimizer.zero_grad()\n      torch.cuda.empty_cache()\n      accumulation_counter = 0\n\n    train_step += 1\n\n  q_scores = dict()\n  model_p.eval()\n  turn_f1s = [[] for _ in range(20)]\n  print('-------------------- Evaluation --------------------')\n  eval_p = EvalProcessOutput()\n  P = []\n  with torch.no_grad():\n    for step, data in tqdm(enumerate(eval_dataloader), position=0, leave=True):\n      input_ids = data.pop('input_ids').to(device)\n      attention_mask = data.pop('attention_mask').to(device)\n      label_tokens = data.pop('label_tokens').to(device)\n      feature_answerability = data.pop('feature_answerability').to(device)\n      features = data.pop('features')\n\n      # run output\n      output, score = model_p(input_ids=input_ids,\n                     attention_mask=attention_mask,\n                     label_tokens=label_tokens,\n                     feature_answerability=feature_answerability,\n                     mode='eval')\n      predicted_text = tokenizer.batch_decode(output.sequences, skip_special_tokens=True)\n      eval_p.process_feature_output(features[0], predicted_text[0], score)\n\n  eval_p.process_output()\n  f1, heq_q, heq_d, mean_f1s, res = run_eval('eval', eval_p)\n  f1_list.append(f1)\n  if f1 > best_eval_f1:\n    print('saving model...')\n    best_eval_checkpoint = f'best_model_{epoch}.pt'\n    save_checkpoint(best_eval_checkpoint)\n    print('model saved successfully')\n    best_eval_f1 = f1\n    with open(eval_res_file, 'w') as f:\n      json.dump(res, f)\n\n  print('Best Eval F1', best_eval_f1)\n  early_stop = all([f1_list[-k] > i for i in f1_list[-k+1:]]) if epoch + 1 >= k else False\n  if early_stop:\n    print('Early Stopping')\n    break\n  model_p.train()\n\n\n\nif args.do_test:\n    model_p.load_state_dict(checkpoint_config['model_dict'])\n    model_p.eval()\n    print('-------------------- Test Time --------------------')\n    test_p = EvalProcessOutput()\n    with torch.no_grad():\n        for step, data in tqdm(enumerate(test_dataloader), position=0, leave=True):\n          input_ids = data.pop('input_ids').to(device)\n          attention_mask = data.pop('attention_mask').to(device)\n          label_tokens = data.pop('label_tokens').to(device)\n          feature_answerability = data.pop('feature_answerability').to(device)\n          features = data.pop('features')\n\n          # run output\n          output, score = model_p(input_ids=input_ids,\n                        attention_mask=attention_mask,\n                        label_tokens=label_tokens,\n                        feature_answerability=feature_answerability,\n                        mode='eval')\n          predicted_text = tokenizer.batch_decode(output.sequences, skip_special_tokens=True)\n          test_p.process_feature_output(features[0], predicted_text[0], score)\n\n\n    test_p.process_output()\n    f1, heq_q, heq_d, mean_f1s, res = run_eval('test', test_p)\n    with open(test_res_file, 'w') as f:\n        json.dump(res, f)\n","metadata":{"id":"lM-isljGiVD8","colab":{"base_uri":"https://localhost:8080/","height":703},"outputId":"d1d51849-7b8f-40fd-e8ef-e49d08c00141","execution":{"iopub.status.busy":"2024-07-08T13:21:26.264351Z","iopub.execute_input":"2024-07-08T13:21:26.264608Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"EPOCH [1/3] | STEP [100/1806] | Loss 1.1641\nEPOCH [1/3] | STEP [200/1806] | Loss 0.9241\nEPOCH [1/3] | STEP [300/1806] | Loss 0.7988\nEPOCH [1/3] | STEP [400/1806] | Loss 0.7405\nEPOCH [1/3] | STEP [500/1806] | Loss 0.6752\nEPOCH [1/3] | STEP [600/1806] | Loss 0.6791\nEPOCH [1/3] | STEP [700/1806] | Loss 0.6402\nEPOCH [1/3] | STEP [800/1806] | Loss 0.6358\nEPOCH [1/3] | STEP [900/1806] | Loss 0.6318\nEPOCH [1/3] | STEP [1000/1806] | Loss 0.621\nEPOCH [1/3] | STEP [1100/1806] | Loss 0.6231\nEPOCH [1/3] | STEP [1200/1806] | Loss 0.6089\nEPOCH [1/3] | STEP [1300/1806] | Loss 0.6169\nEPOCH [1/3] | STEP [1400/1806] | Loss 0.5915\nEPOCH [1/3] | STEP [1500/1806] | Loss 0.5959\nEPOCH [1/3] | STEP [1600/1806] | Loss 0.5817\nEPOCH [1/3] | STEP [1700/1806] | Loss 0.5965\nEPOCH [1/3] | STEP [1800/1806] | Loss 0.618\n-------------------- Evaluation --------------------\n","output_type":"stream"},{"name":"stderr","text":"3675it [28:46,  2.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"f1 score is 0.243096101098699\nHEQ-Q score is 0.20384615384615384\nHEQ-M score is 0.0\nHEQ-D score is 0.0\nEM score is 0.1276923076923077\nUnanswerable score is 0.419811320754717\n[0.4403444845795336, 0.3374695543878552, 0.2633324948299554, 0.18929478785160694, 0.2302177242296849, 0.21770963590263157, 0.16041371982438704, 0.2093665690219475, 0.15129209134808966, 0.20788297012551063, 0.22073011311671292, 0.23650798192756442, 0.20477611932107465, 0.17169790137972582, 0.3040374717281253, 0.02776249003213817, 0.3738977066078778, 0.406249998984375, 0.0, 0.3333333322222222, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\nsaving model...\nmodel saved successfully\nBest Eval F1 0.243096101098699\nEPOCH [2/3] | STEP [101/1806] | Loss 0.5758\nEPOCH [2/3] | STEP [201/1806] | Loss 0.5411\nEPOCH [2/3] | STEP [301/1806] | Loss 0.5346\nEPOCH [2/3] | STEP [401/1806] | Loss 0.5479\nEPOCH [2/3] | STEP [501/1806] | Loss 0.5269\nEPOCH [2/3] | STEP [601/1806] | Loss 0.5211\nEPOCH [2/3] | STEP [701/1806] | Loss 0.5446\nEPOCH [2/3] | STEP [801/1806] | Loss 0.5355\nEPOCH [2/3] | STEP [901/1806] | Loss 0.527\nEPOCH [2/3] | STEP [1001/1806] | Loss 0.5186\nEPOCH [2/3] | STEP [1101/1806] | Loss 0.5229\nEPOCH [2/3] | STEP [1201/1806] | Loss 0.5067\nEPOCH [2/3] | STEP [1301/1806] | Loss 0.5012\nEPOCH [2/3] | STEP [1401/1806] | Loss 0.5048\nEPOCH [2/3] | STEP [1501/1806] | Loss 0.5279\nEPOCH [2/3] | STEP [1601/1806] | Loss 0.5018\nEPOCH [2/3] | STEP [1701/1806] | Loss 0.5269\nEPOCH [2/3] | STEP [1801/1806] | Loss 0.4906\n-------------------- Evaluation --------------------\n","output_type":"stream"},{"name":"stderr","text":"3675it [28:40,  2.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"f1 score is 0.26879865334871966\nHEQ-Q score is 0.21461538461538462\nHEQ-M score is 0.0\nHEQ-D score is 0.0\nEM score is 0.1346153846153846\nUnanswerable score is 0.35377358490566035\n[0.4424112827803949, 0.3804207987982704, 0.31119675319614576, 0.23549679788013508, 0.2656764876125048, 0.22702305581160148, 0.18573445040128375, 0.24374321403414656, 0.17519530025830432, 0.21454470429630138, 0.23047723982299068, 0.23007009409004398, 0.2272345910728886, 0.21981424137037636, 0.3225533687722894, 0.0, 0.34920634862433864, 0.406249998984375, 0.0, 0.3333333322222222, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\nsaving model...\nmodel saved successfully\nBest Eval F1 0.26879865334871966\n","output_type":"stream"}]},{"cell_type":"code","source":"A = eval_p.features","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"save_data(A, 'mmf.pk')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}